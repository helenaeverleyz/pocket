{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pocket Flow","text":"<p>A TypeScript framework for building LLM-powered applications with: - \ud83e\udd16 Intelligent Agents - \ud83e\udde9 Task Decomposition - \ud83d\udcda Retrieval Augmented Generation (RAG) - And more...</p>"},{"location":"#architecture","title":"Architecture","text":"<p>We model LLM workflows as a Nested Directed Graph where:</p> <ul> <li>\ud83d\udd39 Nodes handle atomic LLM tasks</li> <li>\ud83d\udd17 Actions connect nodes (labeled edges) for agent behavior</li> <li>\ud83d\udd04 Flows orchestrate node graphs for task decomposition</li> <li>\ud83d\udce6 Nesting allows flows to be used as nodes</li> <li>\ud83d\udcca Batch processing for data-intensive tasks</li> <li>\u26a1 Async support for parallel execution</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start</li> </ul>"},{"location":"#core-concepts","title":"Core Concepts","text":"<ul> <li>Agent Basics</li> <li>Node Fundamentals</li> <li>Understanding Flows</li> <li>Data Structures</li> </ul>"},{"location":"#features-extensions","title":"Features &amp; Extensions","text":""},{"location":"#basic-features","title":"Basic Features","text":"<ul> <li>Async Processing</li> <li>Batch Operations</li> <li>Communication Patterns</li> <li>Task Decomposition</li> </ul>"},{"location":"#advanced-capabilities","title":"Advanced Capabilities","text":"<ul> <li>Large Language Models</li> <li>MapReduce Workflows</li> <li>Memory &amp; Caching</li> <li>Multi-Agent Systems</li> <li>Parallel Execution</li> <li>Retrieval Augmented Generation</li> <li>Tool Integrations</li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>Sample Applications</li> <li>Conceptual Paradigms</li> <li>Visualizations</li> </ul> <p>Built with TypeScript for production-grade LLM applications</p>"},{"location":"agent/","title":"Agent","text":"<p>For many tasks, we need agents that take dynamic and recursive actions based on the inputs they receive. You can create these agents as Nodes connected by Actions in a directed graph using Flow.</p>"},{"location":"agent/#example-search-agent","title":"Example: Search Agent","text":"<p>This agent: 1. Decides whether to search or answer. 2. If it decides to search, loops back to decide if more searches are needed. 3. Finally answers once enough context has been gathered.</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Placeholder for an LLM call\nasync function callLLM(prompt: string): Promise&lt;string&gt; {\n  // Example function to call a Large Language Model\n  // Return a YAML-like string (or any structured string) in practice\n  return `\n\\`\\`\\`yaml\naction: \"search\"\nreason: \"Need more results\"\nsearch_term: \"Nobel Prize 2024\"\n\\`\\`\\`\n`;\n}\n\n// Placeholder for a web search\nasync function searchWeb(searchTerm: string): Promise&lt;string&gt; {\n  // Example function that interacts with an external API\n  return `Search Results for: ${searchTerm}`;\n}\n\nexport class DecideAction extends BaseNode {\n  // The 'prep' method extracts data from sharedState to pass into execCore\n  public async prep(sharedState: any): Promise&lt;[string, string]&gt; {\n    const context = sharedState.context ?? \"No previous search\";\n    const query = sharedState.query;\n    return [query, context];\n  }\n\n  // The main logic calls callLLM to decide whether to search again or to answer\n  public async execCore(inputs: [string, string]): Promise&lt;any&gt; {\n    const [query, context] = inputs;\n    const prompt = `\nGiven input: ${query}\nPrevious search results: ${context}\nShould I: 1) Search web for more info 2) Answer with current knowledge\nOutput in YAML:\n\\`\\`\\`yaml\naction: search/answer\nreason: Explanation\nsearch_term: search phrase if action is search\n\\`\\`\\`\n    `;\n\n    const resp = await callLLM(prompt);\n\n    // Parse YAML from resp (this is example logic; you'd use a real YAML parser)\n    const yamlStr = resp.split(\"```yaml\")[1]?.split(\"```\")[0]?.trim() || \"\";\n    // Assume the structure is { action, reason, search_term? }\n    const parsed = { action: \"search\", reason: \"Need more results\", search_term: \"Nobel Prize 2024\" };\n    // In a real scenario, you'd do something like:\n    // const parsed = yaml.load(yamlStr); // using js-yaml or similar\n\n    if (parsed.action === \"search\" &amp;&amp; !parsed.search_term) {\n      throw new Error(\"Missing search_term for 'search' action!\");\n    }\n\n    return parsed;\n  }\n\n  public async post(prepResult: [string, string], execResult: any, sharedState: any): Promise&lt;string&gt; {\n    if (execResult.action === \"search\") {\n      sharedState.search_term = execResult.search_term;\n    }\n    return execResult.action;\n  }\n}\n\nexport class SearchWebNode extends BaseNode {\n  public async prep(sharedState: any): Promise&lt;string&gt; {\n    return sharedState.search_term;\n  }\n\n  public async execCore(searchTerm: string): Promise&lt;string&gt; {\n    return await searchWeb(searchTerm);\n  }\n\n  public async post(prepResult: string, execResult: string, sharedState: any): Promise&lt;string&gt; {\n    const previous = sharedState.context || [];\n    sharedState.context = [...previous, { term: prepResult, result: execResult }];\n\n    // Loop back to the DecideAction node\n    return \"decide\";\n  }\n}\n\nexport class DirectAnswer extends BaseNode {\n  public async prep(sharedState: any): Promise&lt;[string, any]&gt; {\n    return [sharedState.query, sharedState.context ?? \"\"];\n  }\n\n  public async execCore(inputs: [string, any]): Promise&lt;string&gt; {\n    const [query, context] = inputs;\n    const prompt = `Context: ${JSON.stringify(context)}\\nAnswer this query: ${query}`;\n    return await callLLM(prompt);\n  }\n\n  public async post(\n    prepResult: [string, any],\n    execResult: string,\n    sharedState: any\n  ): Promise&lt;string&gt; {\n    console.log(\"Answer:\", execResult);\n    sharedState.answer = execResult;\n    return DEFAULT_ACTION; // or any string that indicates the flow is done\n  }\n}\n\n// Connect nodes\nconst decide = new DecideAction();\nconst search = new SearchWebNode();\nconst answer = new DirectAnswer();\n\ndecide.addSuccessor(search, \"search\");\ndecide.addSuccessor(answer, \"answer\");\nsearch.addSuccessor(decide, \"decide\");  // loop back\n\nconst flow = new Flow(decide);\nflow.run({ query: \"Who won the Nobel Prize in Physics 2024?\" });\n</code></pre> <p>In this TypeScript example:</p> <ul> <li>DecideAction checks the shared state, queries an LLM, and sets the next action to either \u201csearch\u201d or \u201canswer.\u201d</li> <li>SearchWebNode performs a web search and appends the result to <code>sharedState.context</code>, then loops back to the decision node.</li> <li>DirectAnswer produces a final answer when enough context has been gathered.</li> </ul> <p>Notes:</p> <ul> <li>YAML Parsing: In the <code>execCore</code> method of <code>DecideAction</code>, replace the placeholder parsing logic with a proper YAML parser like <code>js-yaml</code> to handle the YAML response from <code>callLLM</code>.</li> </ul> <pre><code>import yaml from 'js-yaml';\n\n// Inside execCore\nconst parsed = yaml.load(yamlStr) as { action: string; reason: string; search_term?: string };\n</code></pre> <ul> <li> <p>LLM and Search Implementations: Ensure that the <code>callLLM</code> and <code>searchWeb</code> functions are properly implemented to interact with your chosen LLM service and web search API, respectively.</p> </li> <li> <p>Error Handling: Enhance error handling as needed, especially around the parsing and execution steps to make your agents more robust.</p> </li> </ul> <p>Feel free to adjust the code to fit your actual implementations and expand upon the agent\u2019s capabilities as required!</p>"},{"location":"apps/","title":"Sample Applications","text":"<p>layout: default title: \"Apps\" nav_order: 5 has_children: true</p>"},{"location":"async/","title":"(Advanced) Async","text":"<p>Async Nodes implement <code>prep_async()</code>, <code>exec_async()</code>, <code>exec_fallback_async()</code>, and/or <code>post_async()</code>. This is useful for:</p> <ol> <li>prep_async() </li> <li> <p>For fetching/reading data (files, APIs, DB) in an I/O-friendly way.</p> </li> <li> <p>exec_async() </p> </li> <li> <p>Typically used for async LLM calls.</p> </li> <li> <p>post_async() </p> </li> <li>For awaiting user feedback, coordinating across multi-agents or any additional async steps after <code>exec_async()</code>.</li> </ol> <p>Note: <code>AsyncNode</code> must be wrapped in <code>AsyncFlow</code>. <code>AsyncFlow</code> can also include regular (sync) nodes.</p>"},{"location":"async/#example","title":"Example","text":"<pre><code>class SummarizeThenVerify(AsyncNode):\n    async def prep_async(self, shared):\n        # Example: read a file asynchronously\n        doc_text = await read_file_async(shared[\"doc_path\"])\n        return doc_text\n\n    async def exec_async(self, prep_res):\n        # Example: async LLM call\n        summary = await call_llm_async(f\"Summarize: {prep_res}\")\n        return summary\n\n    async def post_async(self, shared, prep_res, exec_res):\n        # Example: wait for user feedback\n        decision = await gather_user_feedback(exec_res)\n        if decision == \"approve\":\n            shared[\"summary\"] = exec_res\n            return \"approve\"\n        return \"deny\"\n\nsummarize_node = SummarizeThenVerify()\nfinal_node = Finalize()\n\n# Define transitions\nsummarize_node - \"approve\" &gt;&gt; final_node\nsummarize_node - \"deny\"    &gt;&gt; summarize_node  # retry\n\nflow = AsyncFlow(start=summarize_node)\n\nasync def main():\n    shared = {\"doc_path\": \"document.txt\"}\n    await flow.run_async(shared)\n    print(\"Final Summary:\", shared.get(\"summary\"))\n\nasyncio.run(main())\n</code></pre>"},{"location":"batch/","title":"Batch","text":"<p>Batch makes it easier to handle large inputs in one Node or rerun a Flow multiple times. Handy for: - Chunk-based processing (e.g., splitting large texts). - Multi-file processing. - Iterating over lists of params (e.g., user queries, documents, URLs).</p>"},{"location":"batch/#1-batchnode","title":"1. BatchNode","text":"<p>A BatchNode extends <code>BaseNode</code> but changes <code>prepAsync()</code> and <code>execAsync()</code>:</p> <ul> <li><code>prepAsync(shared)</code>: returns an iterable (e.g., array, generator).</li> <li><code>execAsync(item)</code>: called once per item in that iterable.</li> <li><code>postAsync(shared, prepResult, execResultList)</code>: after all items are processed, receives a list of results (<code>execResultList</code>) and returns an Action.</li> </ul>"},{"location":"batch/#example-summarize-a-large-file","title":"Example: Summarize a Large File","text":"<pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Placeholder for an asynchronous file read\nasync function readFileAsync(filePath: string): Promise&lt;string&gt; {\n  const fs = await import('fs/promises');\n  return await fs.readFile(filePath, 'utf-8');\n}\n\n// Placeholder for an asynchronous LLM call\nasync function callLLMAsync(prompt: string): Promise&lt;string&gt; {\n  // Example function to call a Large Language Model asynchronously\n  // Replace with actual implementation\n  return `Summary: ${prompt.substring(0, 50)}...`;\n}\n\nexport class MapSummaries extends BaseNode {\n  // The 'prepAsync' method asynchronously prepares chunks of the large text\n  public async prepAsync(sharedState: any): Promise&lt;string[]&gt; {\n    const content = sharedState.data[\"large_text.txt\"] || \"\";\n    const chunkSize = 10000;\n    const chunks = [];\n    for (let i = 0; i &lt; content.length; i += chunkSize) {\n      chunks.push(content.substring(i, i + chunkSize));\n    }\n    return chunks;\n  }\n\n  // The 'execAsync' method asynchronously summarizes each chunk\n  public async execAsync(chunk: string): Promise&lt;string&gt; {\n    const prompt = `Summarize this chunk in 10 words: ${chunk}`;\n    const summary = await callLLMAsync(prompt);\n    return summary;\n  }\n\n  // The 'postAsync' method combines all summaries and updates the shared state\n  public async postAsync(sharedState: any, prepResult: string[], execResultList: string[]): Promise&lt;string&gt; {\n    const combined = execResultList.join(\"\\n\");\n    sharedState.summary[\"large_text.txt\"] = combined;\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate the node\nconst mapSummaries = new MapSummaries();\n\n// Create a Flow starting with the BatchNode\nconst flow = new Flow(mapSummaries);\n\n// Run the flow with initial shared state\nflow.run({\n  data: {\n    \"large_text.txt\": \"Your very large text content goes here...\"\n  },\n  summary: {}\n});\n</code></pre>"},{"location":"batch/#2-batchflow","title":"2. BatchFlow","text":"<p>A BatchFlow runs a Flow multiple times, each time with different <code>params</code>. Think of it as a loop that replays the Flow for each parameter set.</p>"},{"location":"batch/#example-summarize-many-files","title":"Example: Summarize Many Files","text":"<pre><code>import { BaseNode, BatchFlow, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Placeholder for loading a file\nasync function loadFileAsync(filename: string): Promise&lt;string&gt; {\n  const fs = await import('fs/promises');\n  return await fs.readFile(filename, 'utf-8');\n}\n\n// Placeholder for an asynchronous summarization\nasync function summarizeAsync(content: string): Promise&lt;string&gt; {\n  // Replace with actual summarization logic\n  return `Summary of ${content.substring(0, 20)}...`;\n}\n\nexport class LoadFile extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    return sharedState.filename;\n  }\n\n  public async execAsync(filename: string): Promise&lt;string&gt; {\n    const content = await loadFileAsync(filename);\n    return content;\n  }\n\n  public async postAsync(sharedState: any, prepResult: string, execResult: string): Promise&lt;string&gt; {\n    sharedState.currentContent = execResult;\n    return \"summarize\";\n  }\n}\n\nexport class SummarizeFile extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    return sharedState.currentContent;\n  }\n\n  public async execAsync(content: string): Promise&lt;string&gt; {\n    const summary = await summarizeAsync(content);\n    return summary;\n  }\n\n  public async postAsync(sharedState: any, prepResult: string, execResult: string): Promise&lt;string&gt; {\n    sharedState.summaries[sharedState.filename] = execResult;\n    return DEFAULT_ACTION;\n  }\n}\n\n// Define the per-file Flow\nconst loadFileNode = new LoadFile();\nconst summarizeFileNode = new SummarizeFile();\n\nloadFileNode.addSuccessor(summarizeFileNode, \"summarize\");\nconst fileFlow = new Flow(loadFileNode);\n\n// Define the BatchFlow that iterates over each file\nexport class SummarizeAllFiles extends BatchFlow {\n  public async prepAsync(sharedState: any): Promise&lt;{ filename: string }[]&gt; {\n    const filenames = Object.keys(sharedState.data);\n    return filenames.map(fn =&gt; ({ filename: fn }));\n  }\n\n  public async execAsync(params: { filename: string }): Promise&lt;void&gt; {\n    // Merge current params with shared state\n    const mergedState = { ...sharedState, ...params };\n    await fileFlow.run(mergedState);\n    // Update shared state with summaries\n    sharedState.summaries = sharedState.summaries || {};\n    sharedState.summaries[params.filename] = mergedState.summaries[params.filename];\n  }\n\n  public async postAsync(sharedState: any, prepResult: { filename: string }[], execResultList: void[]): Promise&lt;string&gt; {\n    console.log(\"All files summarized.\");\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate the BatchFlow\nconst summarizeAllFiles = new SummarizeAllFiles();\n\n// Run the BatchFlow with initial shared state\nsummarizeAllFiles.run({\n  data: {\n    \"file1.txt\": \"Content of file 1...\",\n    \"file2.txt\": \"Content of file 2...\",\n    // Add more files as needed\n  },\n  summaries: {}\n});\n</code></pre>"},{"location":"batch/#under-the-hood","title":"Under the Hood","text":"<ol> <li><code>prepAsync(shared)</code> returns a list of parameter objects\u2014e.g., <code>[{ filename: \"file1.txt\" }, { filename: \"file2.txt\" }, ...]</code>.</li> <li>The BatchFlow iterates through each parameter set. For each one:</li> <li>It merges the parameter with the BatchFlow\u2019s own <code>sharedState</code>.</li> <li>It executes the <code>fileFlow</code> (which loads and summarizes the file).</li> <li>After processing all files, the <code>postAsync</code> method logs that all files have been summarized.</li> </ol>"},{"location":"batch/#3-nested-or-multi-level-batches","title":"3. Nested or Multi-Level Batches","text":"<p>You can nest a BatchFlow within another BatchFlow. For example: - Outer BatchFlow: Processes directories, returning parameter sets like <code>{\"directory\": \"/pathA\"}</code>, <code>{\"directory\": \"/pathB\"}</code>, etc. - Inner BatchFlow: Processes files within each directory, returning parameter sets like <code>{\"filename\": \"file1.txt\"}</code>, <code>{\"filename\": \"file2.txt\"}</code>, etc.</p> <p>At each level, BatchFlow merges its own parameters with the parent\u2019s. By the time you reach the innermost node, the final <code>params</code> are a merged result of all parent parameter sets, allowing the flow to maintain the entire context (e.g., directory + filename) seamlessly.</p> <pre><code>import { BaseNode, BatchFlow, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Reuse LoadFile and SummarizeFile from the previous example\n\n// New BatchFlow for processing directories\nexport class DirectoryBatchFlow extends BatchFlow {\n  public async prepAsync(sharedState: any): Promise&lt;{ directory: string }[]&gt; {\n    // Example directories; replace with actual logic to list directories\n    const directories = [\"/path/to/dirA\", \"/path/to/dirB\"];\n    return directories.map(dir =&gt; ({ directory: dir }));\n  }\n\n  public async execAsync(params: { directory: string }): Promise&lt;void&gt; {\n    // Merge current params with shared state\n    const mergedState = { ...sharedState, ...params };\n    // Here you could instantiate another BatchFlow for files within the directory\n    // For simplicity, let's assume files are statically defined\n    mergedState.data = {\n      \"file1.txt\": \"Content of file1 in dirA...\",\n      \"file2.txt\": \"Content of file2 in dirA...\",\n      // Add more files as needed\n    };\n    const summarizeAllFiles = new SummarizeAllFiles();\n    await summarizeAllFiles.run(mergedState);\n    // Update shared state with summaries\n    sharedState.summaries = { ...sharedState.summaries, ...mergedState.summaries };\n  }\n\n  public async postAsync(sharedState: any, prepResult: { directory: string }[], execResultList: void[]): Promise&lt;string&gt; {\n    console.log(\"All directories processed.\");\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate the outer BatchFlow\nconst directoryBatchFlow = new DirectoryBatchFlow();\n\n// Run the nested BatchFlow with initial shared state\ndirectoryBatchFlow.run({\n  summaries: {}\n});\n</code></pre> <p>Explanation: - <code>DirectoryBatchFlow</code>:   - <code>prepAsync(shared)</code>: Provides a list of directories to process.   - <code>execAsync(params)</code>: For each directory, it sets up the files to summarize and invokes the <code>SummarizeAllFiles</code> BatchFlow.   - <code>postAsync(shared, prepResult, execResultList)</code>: Logs completion after all directories are processed.</p> <ul> <li>This nested approach allows you to manage complex processing tasks efficiently, maintaining context across multiple levels of batching.</li> </ul>"},{"location":"batch/#summary","title":"Summary","text":"<p>By converting your Python-based batch processing examples to TypeScript, you can leverage the strong typing and modern JavaScript features inherent to TypeScript. The provided examples demonstrate how to implement BatchNode and BatchFlow classes, handle asynchronous operations, and manage nested batch processes within your <code>pocket.ts</code> framework.</p> <p>Key Points: - BatchNode: Handles iterable data inputs, processing each item individually. - BatchFlow: Manages the execution of a Flow multiple times with different parameters. - Nested Batches: Enables multi-level batching for complex workflows.</p> <p>Next Steps: - Implement Actual Logic: Replace placeholder functions like <code>callLLMAsync</code> and <code>summarizeAsync</code> with real implementations. - Error Handling: Enhance robustness by adding comprehensive error handling. - Testing: Write tests to ensure your batch processes work as expected. - Documentation: Continue documenting other aspects of your framework similarly for consistency.</p> <p>Feel free to further customize these examples to fit your project's specific needs. If you have any questions or need additional assistance, don't hesitate to ask!</p>"},{"location":"communication/","title":"Communication","text":"<p>Nodes and Flows communicate in two ways:</p> <ol> <li>Shared Store \u2013 A global data structure (often an in-memory object) that all nodes can read and write. Every Node's <code>prepAsync()</code> and <code>postAsync()</code> methods receive the same <code>shared</code> store.</li> <li>Params \u2013 Each node and Flow has a unique <code>params</code> object assigned by the parent Flow, typically used as an identifier for tasks. It's strongly recommended to keep parameter keys and values immutable.</li> </ol> <p>If you understand memory management, think of the Shared Store like a heap (shared by all function calls), and Params like a stack (assigned by the caller).</p> <p>Why not use other communication models like Message Passing? </p> <p>At a low-level between nodes, Message Passing works fine for simple DAGs, but in nested or cyclic Flows it becomes unwieldy. A shared store keeps things straightforward. </p> <p>That said, high-level multi-agent patterns like Message Passing and Event-Driven Design can still be layered on top via Async Queues or Pub/Sub in a shared store (see Multi-Agents).</p>"},{"location":"communication/#1-shared-store","title":"1. Shared Store","text":""},{"location":"communication/#overview","title":"Overview","text":"<p>A shared store is typically an in-memory object, like:</p> <pre><code>const shared = { data: {}, summary: {}, config: { /* ... */ } };\n</code></pre> <p>It can also contain local file handlers, database connections, or a combination for persistence. We recommend deciding the data structure or database schema first based on your application's requirements.</p>"},{"location":"communication/#example","title":"Example","text":"<pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Placeholder for an asynchronous LLM call\nasync function callLLM(prompt: string): Promise&lt;string&gt; {\n  // Example implementation\n  return \"This is a summary generated by the LLM.\";\n}\n\nexport class LoadDataNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Simulate reading data from an API or disk\n    sharedState.data[\"my_file.txt\"] = \"Some text content loaded asynchronously.\";\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // No execution needed for loading data\n  }\n\n  public async postAsync(sharedState: any, _: void, __: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\nexport class SummarizeNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    // Access data loaded by LoadDataNode\n    return sharedState.data[\"my_file.txt\"] || \"\";\n  }\n\n  public async execAsync(content: string): Promise&lt;string&gt; {\n    const prompt = `Summarize the following content:\\n${content}`;\n    return await callLLM(prompt);\n  }\n\n  public async postAsync(sharedState: any, _: string, execResult: string): Promise&lt;string&gt; {\n    sharedState.summary[\"my_file.txt\"] = execResult;\n    return \"default\";\n  }\n}\n\n// Instantiate nodes\nconst loadData = new LoadDataNode();\nconst summarize = new SummarizeNode();\n\n// Define the flow\nloadData.addSuccessor(summarize, \"default\");\nconst flow = new Flow(loadData);\n\n// Initial shared state\nconst shared = { data: {}, summary: {}, config: {} };\n\n// Run the flow\nflow.runAsync(shared).then(() =&gt; {\n  console.log(\"Summary:\", shared.summary[\"my_file.txt\"]);\n}).catch(error =&gt; {\n  console.error(\"Flow execution failed:\", error);\n});\n</code></pre> <p>Explanation:</p> <ul> <li>LoadDataNode:</li> <li>prepAsync(): Asynchronously loads data into the <code>sharedState.data</code> object.</li> <li>execAsync(): No operation needed; data loading is handled in <code>prepAsync()</code>.</li> <li> <p>postAsync(): Transitions to the next node by returning <code>DEFAULT_ACTION</code>.</p> </li> <li> <p>SummarizeNode:</p> </li> <li>prepAsync(): Retrieves the loaded data from <code>sharedState</code>.</li> <li>execAsync(): Calls an asynchronous LLM function to summarize the content.</li> <li> <p>postAsync(): Stores the summary in <code>sharedState.summary</code> and transitions to the next action.</p> </li> <li> <p>Flow Execution:</p> </li> <li>The flow starts with <code>LoadDataNode</code>, which loads data.</li> <li>Upon completion, it transitions to <code>SummarizeNode</code> to process and summarize the data.</li> <li>After execution, the summary is available in <code>shared.summary[\"my_file.txt\"]</code>.</li> </ul>"},{"location":"communication/#2-params","title":"2. Params","text":"<p>Params allow you to store per-Node or per-Flow configuration that doesn't need to reside in the shared store. They are:</p> <ul> <li>Immutable during a Node's run cycle (i.e., they don't change mid-<code>prepAsync</code>, <code>execAsync</code>, <code>postAsync</code>).</li> <li>Set via <code>setParams()</code>.</li> <li>Cleared and updated each time a parent Flow calls it.</li> </ul> <p>Only set the uppermost Flow params because others will be overwritten by the parent Flow. If you need to set child node params, see Batch.</p> <p>Typically, Params are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.</p>"},{"location":"communication/#example_1","title":"Example","text":"<pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Placeholder for an asynchronous LLM call\nasync function callLLM(prompt: string): Promise&lt;string&gt; {\n  // Example implementation\n  return `Summary for ${prompt}`;\n}\n\nexport class SummarizeFileNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    // Access the node's params\n    const filename = this.params[\"filename\"];\n    return sharedState.data[filename] || \"\";\n  }\n\n  public async execAsync(content: string): Promise&lt;string&gt; {\n    const prompt = `Summarize the following content:\\n${content}`;\n    return await callLLM(prompt);\n  }\n\n  public async postAsync(sharedState: any, _: string, execResult: string): Promise&lt;string&gt; {\n    const filename = this.params[\"filename\"];\n    sharedState.summary[filename] = execResult;\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate the node\nconst summarizeFile = new SummarizeFileNode();\n\n// Set Node params directly (for testing)\nsummarizeFile.setParams({ filename: \"doc1.txt\" });\n\n// Define the flow\nconst flow = new Flow(summarizeFile);\n\n// Initial shared state\nconst shared = { data: { \"doc1.txt\": \"Content of document 1.\" }, summary: {} };\n\n// Run the flow\nflow.runAsync(shared).then(() =&gt; {\n  console.log(\"Summary:\", shared.summary[\"doc1.txt\"]);\n}).catch(error =&gt; {\n  console.error(\"Flow execution failed:\", error);\n});\n\n// Alternatively, set Flow params (overwrites node params)\nconst flowWithParams = new Flow(summarizeFile);\nflowWithParams.setParams({ filename: \"doc2.txt\" });\nflowWithParams.runAsync({ data: { \"doc2.txt\": \"Content of document 2.\" }, summary: {} }).then(() =&gt; {\n  console.log(\"Summary:\", shared.summary[\"doc2.txt\"]);\n}).catch(error =&gt; {\n  console.error(\"Flow execution failed:\", error);\n});\n</code></pre> <p>Explanation:</p> <ol> <li>SummarizeFileNode:</li> <li>prepAsync(): Retrieves the filename from <code>this.params</code> and fetches the corresponding data from <code>sharedState</code>.</li> <li>execAsync(): Calls an asynchronous LLM function to summarize the content.</li> <li> <p>postAsync(): Stores the summary in <code>sharedState.summary</code> using the filename as the key.</p> </li> <li> <p>Flow Execution:</p> </li> <li>Node Params: Directly setting params on the node (<code>doc1.txt</code>) and running the flow.</li> <li>Flow Params: Setting params on the flow (<code>doc2.txt</code>), which overwrites the node's params, and running the flow.</li> </ol>"},{"location":"communication/#3-shared-store-vs-params","title":"3. Shared Store vs. Params","text":"<p>Think of the Shared Store like a heap and Params like a stack.</p> <ul> <li>Shared Store:</li> <li>Public, Global:<ul> <li>Accessible by all nodes within the flow.</li> </ul> </li> <li>Pre-populated:<ul> <li>Can be initialized with data before the flow starts.</li> </ul> </li> <li>Use Cases:<ul> <li>Data results, large content, or any information multiple nodes need.</li> </ul> </li> <li> <p>Organization:</p> <ul> <li>Structure it carefully, similar to designing a mini schema for clarity and maintainability.</li> </ul> </li> <li> <p>Params:</p> </li> <li>Local, Ephemeral:<ul> <li>Specific to each node or flow instance.</li> </ul> </li> <li>Set by Parent Flows:<ul> <li>Assigned by the flow managing the node.</li> </ul> </li> <li>Use Cases:<ul> <li>Small values like filenames, page numbers, or identifiers.</li> </ul> </li> <li>Characteristics:<ul> <li>Immutable during execution.</li> <li>Do not persist across different nodes and are reset with each new flow run.</li> </ul> </li> </ul>"},{"location":"communication/#key-differences","title":"Key Differences","text":"Feature Shared Store Params Scope Global within the flow Local to the node or flow instance Mutability Mutable Immutable during execution Initialization Can be pre-populated Set dynamically by parent flows Use Cases Large data, results, shared info Identifiers, small configuration data Persistence Can include persistent connections Temporary and ephemeral"},{"location":"communication/#summary","title":"Summary","text":"<p>By converting your Python-based communication examples to TypeScript, you can take full advantage of TypeScript's strong typing and modern asynchronous features. The provided examples demonstrate how to implement Shared Store and Params mechanisms within your <code>pocket.ts</code> framework, ensuring efficient and organized inter-node communication.</p> <p>Key Points:</p> <ul> <li>Shared Store:</li> <li>Facilitates global data sharing across nodes.</li> <li>Ideal for storing large datasets, results, and shared configurations.</li> <li> <p>Must be carefully structured to maintain clarity.</p> </li> <li> <p>Params:</p> </li> <li>Enables passing specific identifiers or configuration data to individual nodes.</li> <li>Ensures immutability during node execution for consistency.</li> <li>Best suited for transient data like filenames or user inputs.</li> </ul> <p>Next Steps:</p> <ul> <li>Implement Actual Logic: Replace placeholder functions like <code>callLLM</code> with real implementations that interact with your services.</li> <li>Enhance Error Handling: Incorporate comprehensive error checks and handling within your nodes to manage potential failures gracefully.</li> <li>Optimize Data Structures: Design and standardize your shared store's data structures to suit various application needs.</li> <li>Expand Documentation: Continue documenting other core abstractions and features of your framework to maintain consistency and clarity.</li> </ul> <p>Feel free to further customize these examples to fit your project's specific requirements. If you have any questions or need additional assistance, don't hesitate to ask!</p>"},{"location":"core_abstraction/","title":"Key Abstractions","text":"<p>layout: default title: \"Core Abstraction\" nav_order: 2 has_children: true</p>"},{"location":"decomp/","title":"Task Decomposition","text":"<p>Many real-world tasks are too complex for a single LLM call. The solution is to decompose them into multiple calls as a Flow of Nodes.</p>"},{"location":"decomp/#example-article-writing","title":"Example: Article Writing","text":"<p>This example demonstrates how to break down the task of writing an article into smaller, manageable steps using TypeScript and the <code>pocket.ts</code> framework.</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Placeholder for an asynchronous LLM call\nasync function callLLM(prompt: string): Promise&lt;string&gt; {\n  // Replace with actual implementation, e.g., API call to an LLM service\n  return `Generated response based on prompt: ${prompt}`;\n}\n\nexport class GenerateOutlineNode extends BaseNode {\n  // The 'prepAsync' method prepares the topic for generating an outline\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    return sharedState.topic;\n  }\n\n  // The 'execAsync' method generates a detailed outline for the article\n  public async execAsync(topic: string): Promise&lt;string&gt; {\n    const prompt = `Create a detailed outline for an article about ${topic}`;\n    const outline = await callLLM(prompt);\n    return outline;\n  }\n\n  // The 'postAsync' method stores the generated outline in the shared store\n  public async postAsync(sharedState: any, _: string, execResult: string): Promise&lt;string&gt; {\n    sharedState.outline = execResult;\n    return DEFAULT_ACTION; // Proceed to the next node\n  }\n}\n\nexport class WriteSectionNode extends BaseNode {\n  // The 'prepAsync' method retrieves the outline from the shared store\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    return sharedState.outline;\n  }\n\n  // The 'execAsync' method writes content based on the outline\n  public async execAsync(outline: string): Promise&lt;string&gt; {\n    const prompt = `Write content based on this outline: ${outline}`;\n    const draft = await callLLM(prompt);\n    return draft;\n  }\n\n  // The 'postAsync' method stores the draft in the shared store\n  public async postAsync(sharedState: any, _: string, execResult: string): Promise&lt;string&gt; {\n    sharedState.draft = execResult;\n    return DEFAULT_ACTION; // Proceed to the next node\n  }\n}\n\nexport class ReviewAndRefineNode extends BaseNode {\n  // The 'prepAsync' method retrieves the draft from the shared store\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    return sharedState.draft;\n  }\n\n  // The 'execAsync' method reviews and improves the draft\n  public async execAsync(draft: string): Promise&lt;string&gt; {\n    const prompt = `Review and improve this draft: ${draft}`;\n    const finalArticle = await callLLM(prompt);\n    return finalArticle;\n  }\n\n  // The 'postAsync' method stores the final article in the shared store\n  public async postAsync(sharedState: any, _: string, execResult: string): Promise&lt;string&gt; {\n    sharedState.final_article = execResult;\n    return DEFAULT_ACTION; // Flow is complete\n  }\n}\n\n// Instantiate nodes\nconst generateOutline = new GenerateOutlineNode();\nconst writeSection = new WriteSectionNode();\nconst reviewAndRefine = new ReviewAndRefineNode();\n\n// Connect nodes to form the flow: GenerateOutline -&gt; WriteSection -&gt; ReviewAndRefine\ngenerateOutline.addSuccessor(writeSection, DEFAULT_ACTION);\nwriteSection.addSuccessor(reviewAndRefine, DEFAULT_ACTION);\n\n// Create the flow starting with the GenerateOutline node\nconst writingFlow = new Flow(generateOutline);\n\n// Define the shared state with the article topic\nconst shared = { topic: \"AI Safety\" };\n\n// Run the flow\nwritingFlow.runAsync(shared).then(() =&gt; {\n  console.log(\"Final Article:\", shared.final_article);\n}).catch(error =&gt; {\n  console.error(\"Flow execution failed:\", error);\n});\n</code></pre>"},{"location":"decomp/#explanation","title":"Explanation:","text":"<ol> <li>GenerateOutlineNode:</li> <li>prepAsync(sharedState):<ul> <li>Retrieves the <code>topic</code> from the <code>sharedState</code> to use in generating the outline.</li> </ul> </li> <li>execAsync(topic):<ul> <li>Calls the <code>callLLM</code> function with a prompt to create a detailed outline for the given topic.</li> </ul> </li> <li> <p>postAsync(sharedState, _, execResult):</p> <ul> <li>Stores the generated outline in <code>sharedState.outline</code>.</li> <li>Returns <code>DEFAULT_ACTION</code> to proceed to the next node in the flow.</li> </ul> </li> <li> <p>WriteSectionNode:</p> </li> <li>prepAsync(sharedState):<ul> <li>Retrieves the <code>outline</code> from the <code>sharedState</code>.</li> </ul> </li> <li>execAsync(outline):<ul> <li>Calls the <code>callLLM</code> function with a prompt to write content based on the provided outline.</li> </ul> </li> <li> <p>postAsync(sharedState, _, execResult):</p> <ul> <li>Stores the draft content in <code>sharedState.draft</code>.</li> <li>Returns <code>DEFAULT_ACTION</code> to proceed to the next node.</li> </ul> </li> <li> <p>ReviewAndRefineNode:</p> </li> <li>prepAsync(sharedState):<ul> <li>Retrieves the <code>draft</code> from the <code>sharedState</code>.</li> </ul> </li> <li>execAsync(draft):<ul> <li>Calls the <code>callLLM</code> function with a prompt to review and improve the draft.</li> </ul> </li> <li> <p>postAsync(sharedState, _, execResult):</p> <ul> <li>Stores the final article in <code>sharedState.final_article</code>.</li> <li>Returns <code>DEFAULT_ACTION</code>, indicating that the flow is complete.</li> </ul> </li> <li> <p>Flow Setup:</p> </li> <li>Node Instantiation:<ul> <li>Creates instances of each node.</li> </ul> </li> <li>Connecting Nodes:<ul> <li>Sets up the flow by connecting the nodes in the order: <code>GenerateOutlineNode</code> \u2192 <code>WriteSectionNode</code> \u2192 <code>ReviewAndRefineNode</code>.</li> </ul> </li> <li>Running the Flow:<ul> <li>Initializes the <code>shared</code> state with the article topic.</li> <li>Runs the flow asynchronously and logs the final article upon completion.</li> </ul> </li> </ol>"},{"location":"decomp/#notes","title":"Notes:","text":"<ul> <li>Asynchronous Operations:</li> <li> <p>All node methods (<code>prepAsync</code>, <code>execAsync</code>, <code>postAsync</code>) are asynchronous to handle operations like API calls to LLMs efficiently.</p> </li> <li> <p>Shared Store Usage:</p> </li> <li> <p>The <code>sharedState</code> object serves as a global store, allowing nodes to share data seamlessly.</p> </li> <li> <p>Flow Execution:</p> </li> <li> <p>The flow begins with generating an outline, proceeds to writing sections based on the outline, and culminates in reviewing and refining the draft to produce the final article.</p> </li> <li> <p>Error Handling:</p> </li> <li>Ensure that the <code>callLLM</code> function and other asynchronous operations include proper error handling to manage potential failures gracefully.</li> </ul> <p>Feel free to adjust the code to fit your actual implementations and expand upon the task decomposition capabilities as required!</p>"},{"location":"essay/","title":"Summarization + QA agent for Paul Graham Essay","text":"<pre><code>from pocketflow import *\nimport openai, os, yaml\n\n# Minimal LLM wrapper\ndef call_llm(prompt):\n    openai.api_key = \"YOUR_API_KEY_HERE\"\n    r = openai.ChatCompletion.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\nshared = {\"data\": {}, \"summary\": {}}\n\n# Load data into shared['data']\nclass LoadData(Node):\n    def prep(self, shared):\n        path = \"./PocketFlow/data/PaulGrahamEssaysLarge\"\n        for fn in os.listdir(path):\n            with open(os.path.join(path, fn), 'r') as f:\n                shared['data'][fn] = f.read()\n    def exec(self, res): pass\n    def post(self, s, pr, er): pass\n\nLoadData().run(shared)\n\n# Summarize one file\nclass SummarizeFile(Node):\n    def prep(self, s): return s['data'][self.params['filename']]\n    def exec(self, content): return call_llm(f\"{content} Summarize in 10 words.\")\n    def post(self, s, pr, sr): s[\"summary\"][self.params['filename']] = sr\n\nnode_summ = SummarizeFile()\nnode_summ.set_params({\"filename\":\"addiction.txt\"})\nnode_summ.run(shared)\n\n# Map-Reduce summarization\nclass MapSummaries(BatchNode):\n    def prep(self, s):\n        text = s['data'][self.params['filename']]\n        return [text[i:i+10000] for i in range(0, len(text), 10000)]\n    def exec(self, chunk):\n        return call_llm(f\"{chunk} Summarize in 10 words.\")\n    def post(self, s, pr, er):\n        s[\"summary\"][self.params['filename']] = [f\"{i}. {r}\" for i,r in enumerate(er)]\n\nclass ReduceSummaries(Node):\n    def prep(self, s): return s[\"summary\"][self.params['filename']]\n    def exec(self, chunks): return call_llm(f\"{chunks} Combine into 10 words summary.\")\n    def post(self, s, pr, sr): s[\"summary\"][self.params['filename']] = sr\n\nmap_summ = MapSummaries()\nreduce_summ = ReduceSummaries()\nmap_summ &gt;&gt; reduce_summ\n\nflow = Flow(start=map_summ)\nflow.set_params({\"filename\":\"before.txt\"})\nflow.run(shared)\n\n# Summarize all files\nclass SummarizeAllFiles(BatchFlow):\n    def prep(self, s): return [{\"filename\":fn} for fn in s['data']]\n\nSummarizeAllFiles(start=flow).run(shared)\n\n# QA agent\nclass FindRelevantFile(Node):\n    def prep(self, s):\n        q = input(\"Enter a question: \")\n        filenames = list(s['summary'].keys())\n        file_summaries = [f\"- '{fn}': {s['summary'][fn]}\" for fn in filenames]\n        return q, filenames, file_summaries\n\n    def exec(self, p):\n        q, filenames, file_summaries = p\n        if not q:\n            return {\"think\":\"no question\", \"has_relevant\":False}\n\n        resp = call_llm(f\"\"\"\nQuestion: {q} \nFind the most relevant file from: {file_summaries}\nIf none, explain why\n\nOutput in code fence:\n```yaml\nthink: &gt;\n    reasoning about relevance\nhas_relevant: true/false\nmost_relevant: filename if relevant\n```\"\"\")\n        yaml_str = resp.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        result = yaml.safe_load(yaml_str)\n\n        # Validate response\n        assert isinstance(result, dict)\n        assert \"think\" in result\n        assert \"has_relevant\" in result\n        assert isinstance(result[\"has_relevant\"], bool)\n\n        if result[\"has_relevant\"]:\n            assert \"most_relevant\" in result\n            assert result[\"most_relevant\"] in filenames\n\n        return result\n\n    def exec_fallback(self, p, exc): return {\"think\":\"error\",\"has_relevant\":False}\n    def post(self, s, pr, res):\n        q, _ = pr\n        if not q:\n            print(\"No question asked\"); return \"end\"\n        if res[\"has_relevant\"]:\n            s[\"question\"], s[\"relevant_file\"] = q, res[\"most_relevant\"]\n            print(\"Relevant file:\", res[\"most_relevant\"])\n            return \"answer\"\n        else:\n            print(\"No relevant file:\", res[\"think\"])\n            return \"retry\"\n\nclass AnswerQuestion(Node):\n    def prep(self, s):\n        return s['question'], s['data'][s['relevant_file']]\n    def exec(self, p):\n        q, txt = p\n        return call_llm(f\"Question: {q}\\nText: {txt}\\nAnswer in 50 words.\")\n    def post(self, s, pr, ex):\n        print(\"Answer:\", ex)\n\nclass NoOp(Node): pass\n\nfrf = FindRelevantFile(max_retries=3)\naq = AnswerQuestion()\nnoop = NoOp()\n\nfrf - \"answer\" &gt;&gt; aq &gt;&gt; frf\nfrf - \"retry\"  &gt;&gt; frf\nfrf - \"end\"    &gt;&gt; noop\n\nqa = Flow(start=frf)\nqa.run(shared)\n</code></pre>"},{"location":"flow/","title":"Flow","text":"<p>A Flow orchestrates how Nodes connect and run, based on Actions returned from each Node's <code>postAsync()</code> method. You can chain Nodes in a sequence or create branching logic depending on the Action string.</p>"},{"location":"flow/#1-action-based-transitions","title":"1. Action-based Transitions","text":"<p>Each Node's <code>postAsync(shared, prepResult, execResult)</code> method returns an Action string. By default, if <code>postAsync()</code> doesn't explicitly return anything, we treat that as <code>\"default\"</code>.</p> <p>You define transitions with the syntax:</p> <ol> <li> <p>Basic default transition: <code>nodeA &gt;&gt; nodeB</code>    This means if <code>nodeA.postAsync()</code> returns <code>\"default\"</code> (or <code>undefined</code>), proceed to <code>nodeB</code>. (Equivalent to <code>nodeA - \"default\" &gt;&gt; nodeB</code>)</p> </li> <li> <p>Named action transition: <code>nodeA - \"actionName\" &gt;&gt; nodeB</code>    This means if <code>nodeA.postAsync()</code> returns <code>\"actionName\"</code>, proceed to <code>nodeB</code>.</p> </li> </ol> <p>It's possible to create loops, branching, or multi-step flows.</p>"},{"location":"flow/#2-creating-a-flow","title":"2. Creating a Flow","text":"<p>A Flow begins with a start node (or another Flow). You create it using <code>new Flow(startNode)</code> to specify the entry point. When you call <code>flow.runAsync(sharedState)</code>, it executes the first node, looks at its <code>postAsync()</code> return Action, follows the corresponding transition, and continues until there's no next node or you explicitly stop.</p>"},{"location":"flow/#example-simple-sequence","title":"Example: Simple Sequence","text":"<p>Here's a minimal flow of two nodes in a chain:</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Define NodeA\nclass NodeA extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Preparation logic for NodeA\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execution logic for NodeA\n  }\n\n  public async postAsync(sharedState: any, _: void, __: void): Promise&lt;string&gt; {\n    // Transition to NodeB\n    return \"default\";\n  }\n}\n\n// Define NodeB\nclass NodeB extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Preparation logic for NodeB\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execution logic for NodeB\n  }\n\n  public async postAsync(sharedState: any, _: void, __: void): Promise&lt;string&gt; {\n    // No further nodes to transition to\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate nodes\nconst nodeA = new NodeA();\nconst nodeB = new NodeB();\n\n// Define the flow connections\nnodeA.addSuccessor(nodeB, \"default\");\n\n// Create the flow starting with nodeA\nconst flow = new Flow(nodeA);\n\n// Initial shared state\nconst sharedState = {};\n\n// Run the flow\nflow.runAsync(sharedState).then(() =&gt; {\n  console.log(\"Flow completed successfully.\");\n}).catch(error =&gt; {\n  console.error(\"Flow execution failed:\", error);\n});\n</code></pre> <ul> <li>When you run the flow, it executes <code>NodeA</code>.</li> <li>Suppose <code>NodeA.postAsync()</code> returns <code>\"default\"</code>.</li> <li>The flow then sees the <code>\"default\"</code> Action is linked to <code>NodeB</code> and runs <code>NodeB</code>.</li> <li>If <code>NodeB.postAsync()</code> returns <code>\"default\"</code> but we didn't define <code>NodeB &gt;&gt; somethingElse</code>, the flow ends there.</li> </ul>"},{"location":"flow/#example-branching-looping","title":"Example: Branching &amp; Looping","text":"<p>Here's a simple expense approval flow that demonstrates branching and looping. The <code>ReviewExpenseNode</code> can return three possible Actions:</p> <ul> <li><code>\"approved\"</code>: Expense is approved, move to payment processing.</li> <li><code>\"needs_revision\"</code>: Expense needs changes, send back for revision.</li> <li><code>\"rejected\"</code>: Expense is denied, finish the process.</li> </ul> <p>We can wire them like this:</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Define ReviewExpenseNode\nclass ReviewExpenseNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare expense data\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute review logic\n  }\n\n  public async postAsync(sharedState: any, prepResult: void, execResult: void): Promise&lt;string&gt; {\n    // Example decision logic\n    const decision = \"approved\"; // Replace with actual decision-making\n    return decision;\n  }\n}\n\n// Define PaymentNode\nclass PaymentNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare payment data\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute payment processing\n  }\n\n  public async postAsync(sharedState: any, prepResult: void, execResult: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\n// Define ReviseExpenseNode\nclass ReviseExpenseNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare revision data\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute revision logic\n  }\n\n  public async postAsync(sharedState: any, prepResult: void, execResult: void): Promise&lt;string&gt; {\n    return \"needs_revision\";\n  }\n}\n\n// Define FinishNode\nclass FinishNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare finish data\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute finish logic\n  }\n\n  public async postAsync(sharedState: any, prepResult: void, execResult: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate nodes\nconst reviewExpense = new ReviewExpenseNode();\nconst payment = new PaymentNode();\nconst reviseExpense = new ReviseExpenseNode();\nconst finish = new FinishNode();\n\n// Define the flow connections\nreviewExpense.addSuccessor(payment, \"approved\");\nreviewExpense.addSuccessor(reviseExpense, \"needs_revision\");\nreviewExpense.addSuccessor(finish, \"rejected\");\n\nreviseExpense.addSuccessor(reviewExpense, \"needs_revision\"); // Loop back for revision\npayment.addSuccessor(finish, \"default\"); // Proceed to finish after payment\n\n// Create the flow starting with reviewExpense\nconst expenseFlow = new Flow(reviewExpense);\n\n// Initial shared state\nconst sharedState = {};\n\n// Run the flow\nexpenseFlow.runAsync(sharedState).then(() =&gt; {\n  console.log(\"Expense flow completed successfully.\");\n}).catch(error =&gt; {\n  console.error(\"Expense flow execution failed:\", error);\n});\n</code></pre>"},{"location":"flow/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TD\n    reviewExpense[Review Expense] --&gt;|approved| payment[Process Payment]\n    reviewExpense --&gt;|needs_revision| reviseExpense[Revise Report]\n    reviewExpense --&gt;|rejected| finish[Finish Process]\n\n    reviseExpense --&gt; reviewExpense\n    payment --&gt; finish\n</code></pre>"},{"location":"flow/#running-individual-nodes-vs-running-a-flow","title":"Running Individual Nodes vs. Running a Flow","text":"<ul> <li> <p><code>node.runAsync(sharedState)</code>:   Just runs that node alone (calls <code>prepAsync()</code>, <code>execAsync()</code>, <code>postAsync()</code>), and returns an Action. Use this for debugging or testing a single node.</p> </li> <li> <p><code>flow.runAsync(sharedState)</code>:   Executes from the start node, follows Actions to the next node, and so on until the flow can't continue (no next node or no next Action). Use this in production to ensure the full pipeline runs correctly.</p> </li> </ul> <p>Warning: <code>node.runAsync(sharedState)</code> does not proceed automatically to the successor and may use incorrect parameters. Always use <code>flow.runAsync(sharedState)</code> in production.</p>"},{"location":"flow/#3-nested-flows","title":"3. Nested Flows","text":"<p>A Flow can act like a Node, enabling powerful composition patterns. This means you can:</p> <ol> <li>Use a Flow as a Node within another Flow's transitions.</li> <li>Combine multiple smaller Flows into a larger Flow for reuse.</li> <li>Node <code>params</code> will be a merging of all parents' <code>params</code>.</li> </ol> <p>Note: While Flow is also a Node, it won't run <code>execAsync()</code>. It will run <code>prepAsync()</code> and <code>postAsync()</code> before and after executing the nodes within the Flow. However, <code>postAsync()</code> always receives <code>undefined</code> for <code>execResult</code>, and should instead retrieve the Flow execution results from the shared store.</p>"},{"location":"flow/#basic-flow-nesting","title":"Basic Flow Nesting","text":"<p>Here's how to connect a nested flow to another node:</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Define sub-flow nodes\nclass NodeA extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare data for NodeA\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute NodeA logic\n  }\n\n  public async postAsync(sharedState: any, prepResult: void, execResult: void): Promise&lt;string&gt; {\n    return \"default\";\n  }\n}\n\nclass NodeB extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare data for NodeB\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute NodeB logic\n  }\n\n  public async postAsync(sharedState: any, prepResult: void, execResult: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\nclass NodeC extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare data for NodeC\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute NodeC logic\n  }\n\n  public async postAsync(sharedState: any, prepResult: void, execResult: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate nodes\nconst nodeA = new NodeA();\nconst nodeB = new NodeB();\nconst nodeC = new NodeC();\n\n// Define the sub-flow: NodeA -&gt; NodeB\nconst subFlow = new Flow(nodeA);\nnodeA.addSuccessor(nodeB, \"default\");\n\n// Connect the sub-flow to NodeC\nsubFlow.addSuccessor(nodeC, \"default\");\n\n// Create the parent flow starting with the sub-flow\nconst parentFlow = new Flow(subFlow);\n\n// Initial shared state\nconst sharedState = {};\n\n// Run the parent flow\nparentFlow.runAsync(sharedState).then(() =&gt; {\n  console.log(\"Parent flow completed successfully.\");\n}).catch(error =&gt; {\n  console.error(\"Parent flow execution failed:\", error);\n});\n</code></pre> <p>When <code>parentFlow.runAsync(sharedState)</code> executes:</p> <ol> <li>It starts the <code>subFlow</code>.</li> <li><code>subFlow</code> runs through its nodes (<code>NodeA</code> then <code>NodeB</code>).</li> <li>After <code>subFlow</code> completes, execution continues to <code>NodeC</code>.</li> </ol>"},{"location":"flow/#example-order-processing-pipeline","title":"Example: Order Processing Pipeline","text":"<p>Here's a practical example that breaks down order processing into nested Flows:</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Payment Processing Nodes\nclass ValidatePaymentNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Validate payment details\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute payment validation\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    // Transition based on validation result\n    const isValid = true; // Replace with actual validation logic\n    return isValid ? \"process_payment\" : \"reject_payment\";\n  }\n}\n\nclass ProcessPaymentNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare payment processing\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute payment processing\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    return \"payment_confirmation\";\n  }\n}\n\nclass PaymentConfirmationNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare payment confirmation\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute payment confirmation\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\n// Inventory Management Nodes\nclass CheckStockNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Check inventory stock\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute stock check\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    const isInStock = true; // Replace with actual stock check logic\n    return isInStock ? \"reserve_items\" : \"out_of_stock\";\n  }\n}\n\nclass ReserveItemsNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare item reservation\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute item reservation\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    return \"update_inventory\";\n  }\n}\n\nclass UpdateInventoryNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare inventory update\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute inventory update\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\n// Shipping Process Nodes\nclass CreateLabelNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare shipping label creation\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute label creation\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    return \"assign_carrier\";\n  }\n}\n\nclass AssignCarrierNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare carrier assignment\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute carrier assignment\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    return \"schedule_pickup\";\n  }\n}\n\nclass SchedulePickupNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare pickup scheduling\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute pickup scheduling\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate payment processing nodes\nconst validatePayment = new ValidatePaymentNode();\nconst processPayment = new ProcessPaymentNode();\nconst paymentConfirmation = new PaymentConfirmationNode();\n\n// Define the payment flow\nvalidatePayment.addSuccessor(processPayment, \"process_payment\");\nvalidatePayment.addSuccessor(new BaseNode(), \"reject_payment\"); // Handle payment rejection as needed\nprocessPayment.addSuccessor(paymentConfirmation, \"payment_confirmation\");\nconst paymentFlow = new Flow(validatePayment);\n\n// Instantiate inventory management nodes\nconst checkStock = new CheckStockNode();\nconst reserveItems = new ReserveItemsNode();\nconst updateInventory = new UpdateInventoryNode();\n\n// Define the inventory flow\ncheckStock.addSuccessor(reserveItems, \"reserve_items\");\ncheckStock.addSuccessor(new BaseNode(), \"out_of_stock\"); // Handle out-of-stock as needed\nreserveItems.addSuccessor(updateInventory, \"update_inventory\");\nconst inventoryFlow = new Flow(checkStock);\n\n// Instantiate shipping process nodes\nconst createLabel = new CreateLabelNode();\nconst assignCarrier = new AssignCarrierNode();\nconst schedulePickup = new SchedulePickupNode();\n\n// Define the shipping flow\ncreateLabel.addSuccessor(assignCarrier, \"assign_carrier\");\nassignCarrier.addSuccessor(schedulePickup, \"schedule_pickup\");\nconst shippingFlow = new Flow(createLabel);\n\n// Connect the flows into the main order processing pipeline\npaymentFlow.addSuccessor(inventoryFlow, \"default\");\ninventoryFlow.addSuccessor(shippingFlow, \"default\");\n\n// Create the master flow starting with the paymentFlow\nconst orderProcessingFlow = new Flow(paymentFlow);\n\n// Initial shared state\nconst sharedState = {\n  orderId: \"12345\",\n  customer: \"John Doe\",\n  items: [\"Item1\", \"Item2\"],\n  paymentDetails: { /* ... */ },\n  inventory: { /* ... */ },\n  shippingDetails: { /* ... */ }\n};\n\n// Run the order processing flow\norderProcessingFlow.runAsync(sharedState).then(() =&gt; {\n  console.log(\"Order processing completed successfully.\");\n}).catch(error =&gt; {\n  console.error(\"Order processing failed:\", error);\n});\n</code></pre>"},{"location":"flow/#flow-diagram_1","title":"Flow Diagram","text":"<pre><code>flowchart LR\n    subgraph orderProcessingFlow[\"Order Processing Flow\"]\n        subgraph paymentFlow[\"Payment Flow\"]\n            A[Validate Payment] --&gt;|process_payment| B[Process Payment]\n            B --&gt;|payment_confirmation| C[Payment Confirmation]\n            A --&gt;|reject_payment| D[Handle Rejection]\n        end\n\n        subgraph inventoryFlow[\"Inventory Flow\"]\n            E[Check Stock] --&gt;|reserve_items| F[Reserve Items]\n            F --&gt;|update_inventory| G[Update Inventory]\n            E --&gt;|out_of_stock| H[Handle Out of Stock]\n        end\n\n        subgraph shippingFlow[\"Shipping Flow\"]\n            I[Create Label] --&gt;|assign_carrier| J[Assign Carrier]\n            J --&gt;|schedule_pickup| K[Schedule Pickup]\n        end\n\n        paymentFlow --&gt; inventoryFlow\n        inventoryFlow --&gt; shippingFlow\n    end\n</code></pre>"},{"location":"flow/#running-individual-nodes-vs-running-a-flow_1","title":"Running Individual Nodes vs. Running a Flow","text":"<ul> <li> <p><code>node.runAsync(sharedState)</code>:   Just runs that node alone (calls <code>prepAsync()</code>, <code>execAsync()</code>, <code>postAsync()</code>), and returns an Action. Use this for debugging or testing a single node.</p> </li> <li> <p><code>flow.runAsync(sharedState)</code>:   Executes from the start node, follows Actions to the next node, and so on until the flow can't continue (no next node or no next Action). Use this in production to ensure the full pipeline runs correctly.</p> </li> </ul> <p>Warning: <code>node.runAsync(sharedState)</code> does not proceed automatically to the successor and may use incorrect parameters. Always use <code>flow.runAsync(sharedState)</code> in production.</p>"},{"location":"flow/#4-specialized-flows","title":"4. Specialized Flows","text":"<p>In addition to the basic <code>Flow</code>, your framework supports specialized flows like <code>AsyncFlow</code>, <code>BatchFlow</code>, and more, allowing for complex orchestration patterns. These specialized flows can handle asynchronous operations, batch processing, and other advanced scenarios seamlessly within your application.</p>"},{"location":"flow/#example-nested-flows-in-order-processing","title":"Example: Nested Flows in Order Processing","text":"<p>Continuing from the previous order processing example, suppose you want to add another layer of processing, such as logging and notification. You can create additional sub-flows or combine existing ones.</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Define LoggingNode\nclass LoggingNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare logging data\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute logging logic\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    console.log(`Order ${sharedState.orderId} processed for ${sharedState.customer}`);\n    return DEFAULT_ACTION;\n  }\n}\n\n// Define NotificationNode\nclass NotificationNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare notification data\n  }\n\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Execute notification logic\n  }\n\n  public async postAsync(sharedState: any, _, __: void): Promise&lt;string&gt; {\n    // Send notification\n    return DEFAULT_ACTION;\n  }\n}\n\n// Instantiate logging and notification nodes\nconst loggingNode = new LoggingNode();\nconst notificationNode = new NotificationNode();\n\n// Connect shippingFlow to logging and notification\nshippingFlow.addSuccessor(loggingNode, \"default\");\nloggingNode.addSuccessor(notificationNode, \"default\");\n\n// Update the master flow to include logging and notification\norderProcessingFlow.addSuccessor(loggingNode, \"default\");\nloggingNode.addSuccessor(notificationNode, \"default\");\n\n// Run the updated flow\norderProcessingFlow.runAsync(sharedState).then(() =&gt; {\n  console.log(\"Order processing with logging and notification completed successfully.\");\n}).catch(error =&gt; {\n  console.error(\"Order processing failed:\", error);\n});\n</code></pre>"},{"location":"flow/#flow-diagram-with-nested-flows","title":"Flow Diagram with Nested Flows","text":"<pre><code>flowchart LR\n    subgraph orderProcessingFlow[\"Order Processing Flow\"]\n        subgraph paymentFlow[\"Payment Flow\"]\n            A[Validate Payment] --&gt;|process_payment| B[Process Payment]\n            B --&gt;|payment_confirmation| C[Payment Confirmation]\n            A --&gt;|reject_payment| D[Handle Rejection]\n        end\n\n        subgraph inventoryFlow[\"Inventory Flow\"]\n            E[Check Stock] --&gt;|reserve_items| F[Reserve Items]\n            F --&gt;|update_inventory| G[Update Inventory]\n            E --&gt;|out_of_stock| H[Handle Out of Stock]\n        end\n\n        subgraph shippingFlow[\"Shipping Flow\"]\n            I[Create Label] --&gt;|assign_carrier| J[Assign Carrier]\n            J --&gt;|schedule_pickup| K[Schedule Pickup]\n        end\n\n        subgraph loggingFlow[\"Logging &amp; Notification Flow\"]\n            L[Logging] --&gt;|default| M[Notification]\n        end\n\n        paymentFlow --&gt; inventoryFlow\n        inventoryFlow --&gt; shippingFlow\n        shippingFlow --&gt; loggingFlow\n    end\n</code></pre>"},{"location":"flow/#summary","title":"Summary","text":"<p>By converting your Python-based Flow examples to TypeScript, you can leverage TypeScript's strong typing and modern asynchronous features. The provided examples demonstrate how to implement Flows, handle Action-based Transitions, and manage Nested Flows within your <code>pocket.ts</code> framework, ensuring efficient and organized workflow orchestration.</p> <p>Key Points:</p> <ul> <li> <p>Flow:   Orchestrates the execution of Nodes based on Actions returned by each node's <code>postAsync()</code> method.</p> </li> <li> <p>Action-based Transitions:   Define how Nodes transition to one another based on specific Action strings.</p> </li> <li> <p>Nested Flows:   Allows flows to act as Nodes within other flows, enabling complex and reusable workflow patterns.</p> </li> <li> <p>Running Flows:   Use <code>flow.runAsync(sharedState)</code> to execute the entire pipeline, ensuring all transitions and nodes are processed correctly.</p> </li> </ul> <p>Next Steps:</p> <ul> <li> <p>Implement Actual Logic:   Replace placeholder functions like <code>callLLM</code> with real implementations that interact with your services.</p> </li> <li> <p>Enhance Error Handling:   Incorporate comprehensive error checks and handling within your nodes to manage potential failures gracefully.</p> </li> <li> <p>Optimize Flow Designs:   Design and structure your Flows to suit various application needs, leveraging nested flows for complex scenarios.</p> </li> <li> <p>Expand Documentation:   Continue documenting other core abstractions and features of your framework to maintain consistency and clarity.</p> </li> </ul> <p>Feel free to further customize these examples to fit your project's specific requirements. If you have any questions or need additional assistance, don't hesitate to ask!</p>"},{"location":"guide/","title":"LLM System Design Guidance","text":"<p>{: .important }</p> <p>Leverage LLMs to help with design and implementation wherever possible, including Node-based flows managed by <code>pocket.ts</code>.</p> <ol> <li>Understand Requirements </li> <li>Clarify the app's tasks and overall workflow.  </li> <li>Determine how data will be accessed (e.g., from files, APIs, or databases).  </li> <li> <p>Specify the type definitions/interfaces you will need in TypeScript.</p> </li> <li> <p>High-Level Flow Design </p> </li> <li>Represent the process as a Directed Graph of Nodes (or sub-Flows) in TypeScript.  <ul> <li>For instance, define each step as a class extending <code>BaseNode</code> from <code>pocket.ts</code>.  </li> </ul> </li> <li>Identify branching logic based on the Action returned from each Node's <code>postAsync()</code>.  </li> <li> <p>Use Batch or Async flows when dealing with large data sets or asynchronous calls, respectively.</p> </li> <li> <p>Shared Memory Structure </p> </li> <li>Decide how you will store and mutate data within <code>sharedState</code>.  </li> <li>For small apps, an in-memory JavaScript/TypeScript object is sufficient.  </li> <li>For larger or persistent requirements, integrate a database driver or external service.  </li> <li> <p>Clearly define your data schema or object structure (TypeScript interfaces or types can help).</p> </li> <li> <p>Implementation </p> </li> <li>Use LLMs for coding tasks (e.g., generating Node classes, writing prompts, or building TypeScript types).  </li> <li>Start with a Flow that orchestrates your nodes.  </li> <li> <p>Keep code minimal and straightforward initially:      <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\nclass ExampleNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    // Prepare data or transform sharedState here\n  }\n  public async execAsync(_: void): Promise&lt;void&gt; {\n    // Actual Node logic or external API call\n  }\n  public async postAsync(sharedState: any, prepResult: any, execResult: any): Promise&lt;string&gt; {\n    // Decide which Action to return\n    return DEFAULT_ACTION;\n  }\n}\n\nconst nodeA = new ExampleNode();\nconst nodeB = new ExampleNode();\nnodeA.addSuccessor(nodeB, DEFAULT_ACTION);\n\nconst flow = new Flow(nodeA);\nflow.runAsync({ /* initial shared state */ })\n    .then(() =&gt; console.log(\"Flow completed\"))\n    .catch(err =&gt; console.error(\"Flow error:\", err));\n</code></pre></p> </li> <li> <p>Optimization </p> </li> <li>Prompt Engineering: Provide clear instructions, inputs, and examples to the LLM for more reliable responses.  </li> <li> <p>Task Decomposition: If tasks grow complex, break them down into more Node classes or sub-Flows. Each Node can handle a smaller part of the overall process.</p> </li> <li> <p>Reliability </p> </li> <li>Structured Output: Ensure you validate the LLM's structured output (e.g., JSON or YAML). Consider libraries like <code>js-yaml</code> or <code>ajv</code> for robust parsing or validation.  </li> <li>Test Cases: Create tests that run each Node independently (<code>node.runAsync()</code>) and/or run entire flows (<code>flow.runAsync()</code>) with well-defined inputs to confirm correctness.  </li> <li>Self-Evaluation: For ambiguous or critical tasks, insert a Node that calls the LLM to review or check prior results within <code>sharedState</code> before continuing to the next stage.</li> </ol>"},{"location":"llm/","title":"LLM Wrappers","text":"<p>We don't provide built-in LLM wrappers. Instead, please implement your own by calling an LLM service from TypeScript (for example, the OpenAI Node.js library). You could also ask a ChatGPT-like assistant to \"implement a <code>callLLM</code> function that takes a prompt and returns the LLM response.\"</p> <p>Below is a TypeScript example using OpenAI:</p> <pre><code>import { Configuration, OpenAIApi } from \"openai\";\n\n/**\n * callLLM\n * \n * An example function that sends a prompt to OpenAI's API and returns the response text.\n * Make sure to store your API key in an environment variable like OPENAI_API_KEY.\n */\nexport async function callLLM(prompt: string): Promise&lt;string&gt; {\n  const config = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n  });\n  const openai = new OpenAIApi(config);\n\n  const response = await openai.createChatCompletion({\n    model: \"gpt-4\",\n    messages: [{ role: \"user\", content: prompt }],\n  });\n\n  // Safely handle the response\n  return response.data.choices?.[0]?.message?.content ?? \"\";\n}\n\n/**\n * Example usage\n */\n// (async () =&gt; {\n//   const reply = await callLLM(\"How are you?\");\n//   console.log(\"LLM reply:\", reply);\n// })();\n</code></pre> <p>Always store the API key in a secure environment variable (e.g., <code>OPENAI_API_KEY</code>) rather than hardcoding it.</p>"},{"location":"llm/#improvements","title":"Improvements","text":"<p>Feel free to enhance your <code>callLLM</code> function as needed. Below are some common patterns:</p>"},{"location":"llm/#1-handling-chat-history","title":"1. Handling Chat History","text":"<p>Rather than a single <code>prompt</code>, you might pass an array of messages:</p> <pre><code>import { Configuration, OpenAIApi, ChatCompletionRequestMessage } from \"openai\";\n\nexport async function callLLMWithHistory(messages: ChatCompletionRequestMessage[]): Promise&lt;string&gt; {\n  const config = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n  });\n  const openai = new OpenAIApi(config);\n\n  const response = await openai.createChatCompletion({\n    model: \"gpt-4\",\n    messages: messages,\n  });\n\n  return response.data.choices?.[0]?.message?.content ?? \"\";\n}\n</code></pre>"},{"location":"llm/#2-adding-in-memory-caching","title":"2. Adding In-Memory Caching","text":"<p>Using the lru-cache or a similar library, you can cache responses to avoid repeating identical calls. Below is a sketch:</p> <pre><code>import LRU from \"lru-cache\";\nimport { Configuration, OpenAIApi } from \"openai\";\n\nconst llmCache = new LRU&lt;string, string&gt;({ max: 1000 });\n\nexport async function cachedCallLLM(prompt: string, useCache: boolean): Promise&lt;string&gt; {\n  // If caching is enabled and a cached value exists, return it\n  if (useCache &amp;&amp; llmCache.has(prompt)) {\n    return llmCache.get(prompt) as string;\n  }\n\n  // Otherwise, call the LLM\n  const config = new Configuration({\n    apiKey: process.env.OPENAI_API_KEY,\n  });\n  const openai = new OpenAIApi(config);\n\n  const response = await openai.createChatCompletion({\n    model: \"gpt-4\",\n    messages: [{ role: \"user\", content: prompt }],\n  });\n\n  const text = response.data.choices?.[0]?.message?.content ?? \"\";\n\n  // Cache the result if desired\n  if (useCache) {\n    llmCache.set(prompt, text);\n  }\n  return text;\n}\n</code></pre> <p>\u26a0\ufe0f Caching conflicts with Node retries, since retries yield the same result. You could only use cached results on the first attempt and bypass the cache on subsequent retries.</p>"},{"location":"llm/#3-enabling-logging","title":"3. Enabling Logging","text":"<p>Use your logging framework (e.g., winston, pino, or Node.js <code>console</code>) to track prompts and responses:</p> <pre><code>export async function callLLMWithLogging(prompt: string): Promise&lt;string&gt; {\n  console.info(`Prompt: ${prompt}`);\n  // ...Perform the call as shown above\n  // Example:\n  const reply = await callLLM(prompt);\n  console.info(`Response: ${reply}`);\n  return reply;\n}\n</code></pre>"},{"location":"llm/#why-not-provide-built-in-llm-wrappers","title":"Why Not Provide Built-in LLM Wrappers?","text":"<p>It's considered bad practice to bundle specific LLM implementations inside a generic framework, for reasons such as:</p> <ul> <li>Frequent API Changes: LLM providers evolve their APIs rapidly. Hardcoding them makes maintenance difficult.  </li> <li>Flexibility: You might switch vendors (OpenAI, Anthropic, or local models) or need to fine-tune your own models.  </li> <li>Optimizations: You may need caching, request batching, or streaming\u2014custom solutions are often essential.</li> </ul>"},{"location":"mapreduce/","title":"Map Reduce","text":"<p>Process large inputs by splitting them into chunks (using something like a BatchNode-style approach), then combining the result in a reduce step.</p>"},{"location":"mapreduce/#example-document-summarization","title":"Example: Document Summarization","text":"<pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// Placeholder LLM call that takes a prompt and returns a string\nasync function callLLM(prompt: string): Promise&lt;string&gt; {\n  // Replace with your actual LLM logic (OpenAI, local model, etc.)\n  return `Summary: ${prompt.substring(0, 60)}...`;\n}\n\n/**\n * MapSummaries Node:\n *  - Splits a large text into chunks (prepAsync)\n *  - Summarizes each chunk (execAsync)\n *  - Collects the chunk summaries into an array (postAsync)\n */\nexport class MapSummaries extends BaseNode {\n  // The 'prepAsync' method chunks the text\n  public async prepAsync(sharedState: any): Promise&lt;string[]&gt; {\n    const text = sharedState.text ?? \"\";\n    const chunkSize = 10000;\n    const chunks: string[] = [];\n    for (let i = 0; i &lt; text.length; i += chunkSize) {\n      chunks.push(text.slice(i, i + chunkSize));\n    }\n    return chunks;\n  }\n\n  // The 'execAsync' method calls the LLM for each chunk\n  public async execAsync(chunk: string): Promise&lt;string&gt; {\n    return await callLLM(`Summarize this chunk: ${chunk}`);\n  }\n\n  // The 'postAsync' method saves each summarized chunk into sharedState\n  public async postAsync(sharedState: any, prepResult: string[], execResultList: string[]): Promise&lt;string&gt; {\n    sharedState.summaries = execResultList;\n    return DEFAULT_ACTION; // Transition to the reduce node\n  }\n}\n\n/**\n * ReduceSummaries Node:\n *  - Takes the array of chunk summaries\n *  - Merges them into one final summary\n */\nexport class ReduceSummaries extends BaseNode {\n  // The 'prepAsync' method retrieves the chunk summaries\n  public async prepAsync(sharedState: any): Promise&lt;string[]&gt; {\n    return sharedState.summaries ?? [];\n  }\n\n  // The 'execAsync' method calls the LLM to combine the chunk summaries\n  public async execAsync(summaries: string[]): Promise&lt;string&gt; {\n    const prompt = `Combine these summaries:\\n${summaries.join(\"\\n\")}`;\n    return await callLLM(prompt);\n  }\n\n  // The 'postAsync' method saves the final summary\n  public async postAsync(sharedState: any, prepResult: string[], execResult: string): Promise&lt;string&gt; {\n    sharedState.final_summary = execResult;\n    return DEFAULT_ACTION; // Flow ends here by default\n  }\n}\n\n// Instantiate the Map (split+summaries) and Reduce nodes\nconst mapNode = new MapSummaries();\nconst reduceNode = new ReduceSummaries();\n\n// Connect mapNode to reduceNode\nmapNode.addSuccessor(reduceNode, DEFAULT_ACTION);\n\n// Create the Flow\nconst summarizeFlow = new Flow(mapNode);\n\n// Example usage\n(async () =&gt; {\n  const shared: any = {\n    text: \"Very large text content goes here...\",\n  };\n  await summarizeFlow.runAsync(shared);\n  console.log(\"Final summary:\", shared.final_summary);\n})();\n</code></pre> <p>In this Map-Reduce approach:</p> <ul> <li>MapSummaries:</li> <li>prepAsync: Splits the input text (<code>sharedState.text</code>) into chunks.  </li> <li>execAsync: Summarizes each chunk via <code>callLLM(...)</code>.  </li> <li> <p>postAsync: Saves the individual chunk summaries into <code>sharedState.summaries</code>.</p> </li> <li> <p>ReduceSummaries:</p> </li> <li>prepAsync: Retrieves the list of chunk summaries.  </li> <li>execAsync: Combines them into one final summary with another LLM call.  </li> <li>postAsync: Saves the final summary in <code>sharedState.final_summary</code>.</li> </ul> <p>By chaining these Nodes, you achieve a straightforward Map-Reduce flow for processing large text inputs.</p>"},{"location":"memory/","title":"Chat Memory","text":"<p>Multi-turn conversations require memory management to maintain context while avoiding overwhelming the LLM.</p>"},{"location":"memory/#1-naive-approach-full-history","title":"1. Naive Approach: Full History","text":"<p>Sending the full chat history may overwhelm LLMs.</p> <pre><code>class ChatNode(Node):\n    def prep(self, shared):\n        if \"history\" not in shared:\n            shared[\"history\"] = []\n        user_input = input(\"You: \")\n        return shared[\"history\"], user_input\n\n    def exec(self, inputs):\n        history, user_input = inputs\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"}]\n        for h in history:\n            messages.append(h)\n        messages.append({\"role\": \"user\", \"content\": user_input})\n        response = call_llm(messages)\n        return response\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"history\"].append({\"role\": \"user\", \"content\": prep_res[1]})\n        shared[\"history\"].append({\"role\": \"assistant\", \"content\": exec_res})\n        return \"continue\"\n\nchat = ChatNode()\nchat - \"continue\" &gt;&gt; chat\nflow = Flow(start=chat)\n</code></pre>"},{"location":"memory/#2-improved-memory-management","title":"2. Improved Memory Management","text":"<p>We can: 1. Limit the chat history to the most recent 4. 2. Use vector search to retrieve relevant exchanges beyond the last 4.</p> <pre><code>class ChatWithMemory(Node):\n    def prep(self, s):\n        # Initialize shared dict\n        s.setdefault(\"history\", [])\n        s.setdefault(\"memory_index\", None)\n\n        user_input = input(\"You: \")\n\n        # Retrieve relevant past if we have enough history and an index\n        relevant = []\n        if len(s[\"history\"]) &gt; 8 and s[\"memory_index\"]:\n            idx, _ = search_index(s[\"memory_index\"], get_embedding(user_input), top_k=2)\n            relevant = [s[\"history\"][i[0]] for i in idx]\n\n        return {\"user_input\": user_input, \"recent\": s[\"history\"][-8:], \"relevant\": relevant}\n\n    def exec(self, c):\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n        # Include relevant history if any\n        if c[\"relevant\"]:\n            messages.append({\"role\": \"system\", \"content\": f\"Relevant: {c['relevant']}\"})\n        # Add recent history and the current user input\n        messages += c[\"recent\"] + [{\"role\": \"user\", \"content\": c[\"user_input\"]}]\n        return call_llm(messages)\n\n    def post(self, s, pre, ans):\n        # Update chat history\n        s[\"history\"] += [\n            {\"role\": \"user\", \"content\": pre[\"user_input\"]},\n            {\"role\": \"assistant\", \"content\": ans}\n        ]\n\n        # When first reaching 8 messages, create index\n        if len(s[\"history\"]) == 8:\n            embeddings = []\n            for i in range(0, 8, 2):\n                e = s[\"history\"][i][\"content\"] + \" \" + s[\"history\"][i+1][\"content\"]\n                embeddings.append(get_embedding(e))\n            s[\"memory_index\"] = create_index(embeddings)\n\n        # Embed older exchanges once we exceed 8 messages\n        elif len(s[\"history\"]) &gt; 8:\n            pair = s[\"history\"][-10:-8]\n            embedding = get_embedding(pair[0][\"content\"] + \" \" + pair[1][\"content\"])\n            s[\"memory_index\"].add(np.array([embedding]).astype('float32'))\n\n        print(f\"Assistant: {ans}\")\n        return \"continue\"\n\nchat = ChatWithMemory()\nchat - \"continue\" &gt;&gt; chat\nflow = Flow(start=chat)\nflow.run({})\n</code></pre>"},{"location":"multi_agent/","title":"(Advanced) Multi-Agents","text":"<p>Sometimes, you want multiple agents (or flows) working together, each performing different tasks or roles. They can communicate by passing messages or updating shared states. Below are some TypeScript examples.</p>"},{"location":"multi_agent/#example-1-agent-communication-with-a-shared-queue","title":"Example 1: Agent Communication with a Shared Queue","text":"<p>Here's how to implement communication using a queue-like structure in Node.js. The agent listens for messages, processes them, and loops back to await more. We will simulate an asynchronous message queue using standard JavaScript patterns (e.g., an array plus <code>setInterval</code> or an event-based approach).</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n// We'll define a simple queue interface\ninterface MessageQueue {\n  messages: string[];\n  // optional signals or a real concurrency approach\n}\n\n// This is our AgentNode, which reads a message from the queue each time it runs.\n// For demonstration, we poll the queue at intervals to simulate asynchronous arrival of messages.\nexport class AgentNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string | null&gt; {\n    const messageQueue: MessageQueue = this.params[\"messages\"] as MessageQueue;\n    if (!messageQueue || !Array.isArray(messageQueue.messages)) {\n      throw new Error(\"Invalid message queue\");\n    }\n\n    // Wait until there's at least one message or return null if no messages are pending\n    if (messageQueue.messages.length === 0) {\n      return null;\n    }\n\n    // Dequeue the first message\n    return messageQueue.messages.shift() || null;\n  }\n\n  public async execAsync(message: string | null): Promise&lt;string | null&gt; {\n    if (message === null) {\n      // No new message\n      return null;\n    }\n\n    // Process or log the message\n    console.log(`Agent received: ${message}`);\n    // You could also call an LLM or do more sophisticated processing here\n    return message;\n  }\n\n  public async postAsync(sharedState: any, prepResult: string | null, execResult: string | null): Promise&lt;string&gt; {\n    // We can continue if there's more work to do; otherwise, we might wait or exit.\n    // For this example, we just loop forever (polling), so we return \"loop\"\n    return \"loop\";\n  }\n}\n\n// Example usage\n(async () =&gt; {\n  // Our simulated queue\n  const messageQueue: MessageQueue = {\n    messages: [],\n  };\n\n  // Create the agent node\n  const agent = new AgentNode();\n  // Connect the agent node to itself on the \"loop\" action\n  agent.addSuccessor(agent, \"loop\");\n\n  // Create a flow starting from our agent\n  const flow = new Flow(agent);\n\n  // Set the flow params so the agent node knows about the queue\n  flow.setParams({ messages: messageQueue });\n\n  // We'll also define a simple message generator that appends to the queue periodically\n  setInterval(() =&gt; {\n    const timestamp = Date.now();\n    const sampleMessages = [\n      \"System status: all systems operational\",\n      \"Memory usage: normal\",\n      \"Network connectivity: stable\",\n      \"Processing load: optimal\",\n    ];\n    const msg = `${sampleMessages[timestamp % sampleMessages.length]} | timestamp_${timestamp}`;\n    messageQueue.messages.push(msg);\n  }, 1000);\n\n  // Run the flow\n  const shared = {};\n  // Because it loops indefinitely, this flow never truly ends unless we forcibly stop it.\n  // In a real app, you'd have a stopping condition or signal.\n  flow.runAsync(shared).catch((err) =&gt; {\n    console.error(\"Flow execution failed:\", err);\n  });\n})();\n</code></pre> <p>Explanation: - We store messages in <code>messageQueue.messages</code>. - Each agent run cycle uses <code>prepAsync</code> to dequeue one message. - The node returns <code>\"loop\"</code> in <code>postAsync</code>, telling the flow to run the same node again for the next message.</p>"},{"location":"multi_agent/#example-2-interactive-multi-agent-example-taboo-game","title":"Example 2: Interactive Multi-Agent Example: Taboo Game","text":"<p>Here's a more complex setup with two agents (a \"Hinter\" and a \"Guesser\") playing a simplified word-guessing game. They communicate via two queues: - <code>hinterQueue</code> sends guesses to the Hinter agent, - <code>guesserQueue</code> sends hints to the Guesser agent.</p> <p>Warning: Below is a conceptual example. In a real Node.js environment, you might orchestrate concurrency differently (e.g., using <code>Promise.all</code>, or a dedicated event system).</p> <pre><code>import { BaseNode, Flow } from \"../src/pocket\";\n\n// Placeholder LLM function (replace with real calls as needed)\nasync function callLLM(prompt: string): Promise&lt;string&gt; {\n  // For demonstration\n  return `LLM says: ${prompt.substring(0, 60)}`;\n}\n\n/** \n * AsyncHinter:\n *  1) Waits for a guess from the guesser (via hinterQueue).\n *  2) Generates a new hint while avoiding certain forbidden words.\n *  3) Sends the hint to guesserQueue.\n */\nexport class AsyncHinter extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;{\n    guess: string;\n    target: string;\n    forbidden: string[];\n    pastGuesses: string[];\n  } | null&gt; {\n    // Dequeue guess\n    const hinterQueue = sharedState.hinterQueue as string[];\n    if (!Array.isArray(hinterQueue)) throw new Error(\"hinterQueue not found\");\n\n    if (hinterQueue.length === 0) {\n      return null; // no new guess in queue\n    }\n    const guess = hinterQueue.shift() as string;\n\n    // If guess == \"GAME_OVER\", we can end the Hinter agent\n    if (guess === \"GAME_OVER\") {\n      return null;\n    }\n\n    return {\n      guess,\n      target: sharedState.target_word,\n      forbidden: sharedState.forbidden_words,\n      pastGuesses: sharedState.past_guesses ?? [],\n    };\n  }\n\n  public async execAsync(inputs: {\n    guess: string;\n    target: string;\n    forbidden: string[];\n    pastGuesses: string[];\n  } | null): Promise&lt;string | null&gt; {\n    if (!inputs) {\n      return null; // means we should end\n    }\n    const { guess, target, forbidden, pastGuesses } = inputs;\n\n    // The prompt for generating a hint from the LLM\n    let prompt = `Generate a 1-sentence hint for the word \"${target}\". Avoid these words: ${forbidden.join(\", \")}. `;\n    if (guess !== \"\") {\n      prompt += `Previous guess was: \"${guess}\". `;\n    }\n    if (pastGuesses.length) {\n      prompt += `Past wrong guesses: ${pastGuesses.join(\", \")}. `;\n    }\n    prompt += \"Hint: use at most 5 words.\";\n\n    const hint = await callLLM(prompt);\n    console.log(`\\nHinter: Here's your hint -&gt; ${hint}`);\n    return hint;\n  }\n\n  public async postAsync(\n    sharedState: any,\n    prepResult: {\n      guess: string;\n      target: string;\n      forbidden: string[];\n      pastGuesses: string[];\n    } | null,\n    execResult: string | null\n  ): Promise&lt;string&gt; {\n    // If no inputs or execResult, game is over or no messages left\n    if (!prepResult || execResult === null) {\n      return \"end\"; \n    }\n\n    // Send the generated hint to guesserQueue\n    const guesserQueue = sharedState.guesserQueue as string[];\n    guesserQueue.push(execResult);\n\n    return \"continue\"; \n  }\n}\n\n/** \n * AsyncGuesser:\n *  1) Waits for a hint from guesserQueue.\n *  2) Generates a guess.\n *  3) Checks correctness. If correct, game ends; else adds guess to pastGuesses and re-queues for Hinter.\n */\nexport class AsyncGuesser extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;{\n    hint: string;\n    pastGuesses: string[];\n    target: string;\n  } | null&gt; {\n    const guesserQueue = sharedState.guesserQueue as string[];\n    if (!Array.isArray(guesserQueue)) throw new Error(\"guesserQueue not found\");\n\n    if (guesserQueue.length === 0) {\n      return null;\n    }\n    const hint = guesserQueue.shift() as string;\n    return {\n      hint,\n      pastGuesses: sharedState.past_guesses ?? [],\n      target: sharedState.target_word,\n    };\n  }\n\n  public async execAsync(inputs: {\n    hint: string;\n    pastGuesses: string[];\n    target: string;\n  } | null): Promise&lt;string | null&gt; {\n    if (!inputs) {\n      return null;\n    }\n\n    const { hint, pastGuesses, target } = inputs;\n    let prompt = `We have hint: \"${hint}\". Past wrong guesses: ${pastGuesses.join(\", \")}. Make a new single-word guess:`;\n    // In reality, you'd refine this logic or call an actual LLM\n    const guess = await callLLM(prompt);\n    console.log(`Guesser: I guess it's -&gt; ${guess}`);\n    return guess;\n  }\n\n  public async postAsync(\n    sharedState: any,\n    prepResult: { hint: string; pastGuesses: string[]; target: string } | null,\n    execResult: string | null\n  ): Promise&lt;string&gt; {\n    if (!prepResult || execResult === null) {\n      return \"end\";\n    }\n\n    // Check correctness\n    const guessLower = execResult.trim().toLowerCase();\n    const targetLower = prepResult.target.trim().toLowerCase();\n    if (guessLower === targetLower) {\n      console.log(\"Game Over -&gt; Correct guess!\");\n      // Signal the hinter to stop\n      const hinterQueue = sharedState.hinterQueue as string[];\n      hinterQueue.push(\"GAME_OVER\");\n      return \"end\";\n    }\n\n    // If guess is wrong, add to pastGuesses\n    if (!sharedState.past_guesses) {\n      sharedState.past_guesses = [];\n    }\n    sharedState.past_guesses.push(execResult);\n\n    // Send guess to the Hinter for feedback\n    const hinterQueue = sharedState.hinterQueue as string[];\n    hinterQueue.push(execResult);\n\n    return \"continue\";\n  }\n}\n\n// Example usage\n(async () =&gt; {\n  const shared = {\n    target_word: \"nostalgia\",\n    forbidden_words: [\"memory\", \"past\", \"remember\", \"feeling\", \"longing\"],\n    hinterQueue: [] as string[],\n    guesserQueue: [] as string[],\n    past_guesses: [] as string[],\n  };\n\n  console.log(\"Game starting!\");\n  console.log(`Target word: ${shared.target_word}`);\n  console.log(`Forbidden words: ${shared.forbidden_words}`);\n\n  // Initialize by sending an empty guess to Hinter\n  shared.hinterQueue.push(\"\");\n\n  const hinter = new AsyncHinter();\n  const guesser = new AsyncGuesser();\n\n  // In pocket.ts, you might have AsyncFlow (if your BaseNode variants are async).\n  // For demonstration, assume Flow can handle async as well. \n  const hinterFlow = new Flow(hinter);\n  const guesserFlow = new Flow(guesser);\n\n  // Connect each node to itself to allow multiple turns\n  hinter.addSuccessor(hinter, \"continue\");\n  guesser.addSuccessor(guesser, \"continue\");\n\n  // Start both flows concurrently\n  // Typically you'd want a coordination mechanism like Promise.all or a dedicated runner\n  hinterFlow.runAsync(shared).catch((err) =&gt; console.error(\"Hinter flow failed:\", err));\n  guesserFlow.runAsync(shared).catch((err) =&gt; console.error(\"Guesser flow failed:\", err));\n})();\n</code></pre> <p>Explanation: 1. Queues:    - <code>hinterQueue</code> carries guesses from the Guesser to the Hinter.    - <code>guesserQueue</code> carries hints from the Hinter to the Guesser. 2. AsyncHinter:    - Awaits a guess. If <code>\"GAME_OVER\"</code>, the agent ends. Otherwise, it generates a new hint and puts it into <code>guesserQueue</code>. 3. AsyncGuesser:    - Pulls a hint from <code>guesserQueue</code>, generates a guess, and checks if correct.    - If correct, ends the game; otherwise, pushes the guess back to <code>hinterQueue</code>. 4. Loops:    - Each agent calls <code>addSuccessor(node, \"continue\")</code> to keep running in a loop until the game finishes or no more messages are available.</p>"},{"location":"multi_agent/#building-multi-agent-systems","title":"Building Multi-Agent Systems","text":"<ul> <li>Shared State: Store data structures like queues, agent statuses, or global game state in <code>sharedState</code>.</li> <li>Flow or AsyncFlow: Each agent can be a node or sub-flow that loops for repeated interactions.</li> <li>Communication: Use shared structures (queues or specialized abstractions) to pass messages or signals between agents.</li> </ul> <p>This design enables flexible multi-agent architectures, letting you break down complex tasks among multiple specialized agents.</p>"},{"location":"node/","title":"Node","text":"<p>A Node is the smallest building block. Each Node (in TypeScript, commonly extending <code>BaseNode</code>) has three main method overrides (all optional):</p> <ol> <li><code>prepAsync(sharedState)</code> </li> <li>A reliable step for preprocessing data from the <code>sharedState</code>.  </li> <li>Examples: query a DB, read files, or serialize data into a string.  </li> <li> <p>Returns <code>prepResult</code>, which is passed to <code>execAsync()</code> and <code>postAsync()</code>.</p> </li> <li> <p><code>execAsync(prepResult)</code> </p> </li> <li>The main execution step, with optional retries and error handling configured in the node's options.  </li> <li>Examples: primarily for LLM calls, but can also be used for remote APIs.  </li> <li>Note: If retries are enabled, ensure an idempotent implementation.  </li> <li>Important: This method should not write to <code>sharedState</code>. If you need data from <code>sharedState</code>, gather it in <code>prepAsync()</code> and pass it along as <code>prepResult</code>.  </li> <li> <p>Returns <code>execResult</code>, which is passed to <code>postAsync()</code>.</p> </li> <li> <p><code>postAsync(sharedState, prepResult, execResult)</code> </p> </li> <li>A reliable postprocessing step to write results back to the <code>sharedState</code> and decide the next Action.  </li> <li>Examples: update DB, change states, log results, decide next Action.  </li> <li>Returns a string specifying the next Action (e.g. <code>\"default\"</code> if none is specified).</li> </ol> <p>Not all Nodes need all three steps. You could implement only <code>prepAsync()</code> if you just need to prepare data without calling an LLM.</p>"},{"location":"node/#fault-tolerance-retries","title":"Fault Tolerance &amp; Retries","text":"<p>Nodes can retry <code>execAsync()</code> if it throws an exception. You configure this by passing options (e.g., <code>maxRetries</code>, <code>wait</code>) to the constructor (or use a property). For instance:</p> <pre><code>const myNode = new SummarizeFile({ maxRetries: 3, wait: 10 });\n</code></pre> <p>This means: - <code>maxRetries = 3</code>: Up to 3 attempts total. - <code>wait = 10</code>: Wait 10 seconds before each retry.  </p> <p>When an exception occurs in <code>execAsync()</code>, the Node automatically retries until: - It either succeeds, or - The Node has retried <code>maxRetries - 1</code> times already and fails on the last attempt.</p>"},{"location":"node/#graceful-fallback","title":"Graceful Fallback","text":"<p>If you want to handle errors gracefully instead of throwing, override:</p> <pre><code>public async execFallbackAsync(\n  sharedState: any,\n  prepResult: any,\n  error: Error\n): Promise&lt;any&gt; {\n  throw error; // Default just rethrows\n}\n</code></pre> <p>By default, it rethrows the exception. But you can return a fallback result instead, which becomes the <code>execResult</code> passed to <code>postAsync()</code>.</p>"},{"location":"node/#example-summarize-a-file","title":"Example: Summarize a File","text":"<p>Below is a Node that reads file content from <code>sharedState</code>, calls an LLM to summarize it, and saves the result back:</p> <pre><code>import { BaseNode, DEFAULT_ACTION } from \"../src/pocket\";\nimport { callLLM } from \"../path/to/your/llm-wrapper\";\n\nexport class SummarizeFile extends BaseNode {\n  // constructor to accept node config (like maxRetries)\n  constructor(private config?: { maxRetries?: number; wait?: number }) {\n    super(config);\n  }\n\n  // prepAsync: read data from sharedState\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    const filename = this.params[\"filename\"] as string;\n    const fileContent = sharedState.data?.[filename];\n    return fileContent ?? \"\";\n  }\n\n  // execAsync: make the LLM call with the prepared file content\n  public async execAsync(fileContent: string): Promise&lt;string&gt; {\n    if (!fileContent) {\n      throw new Error(\"Empty file content!\");\n    }\n    const prompt = `Summarize this text in ~10 words:\\n${fileContent}`;\n    const summary = await callLLM(prompt); // might fail\n    return summary;\n  }\n\n  // execFallbackAsync: provide a default summary if there's an error\n  public async execFallbackAsync(\n    sharedState: any,\n    prepResult: string,\n    err: Error\n  ): Promise&lt;string&gt; {\n    console.error(\"LLM call failed:\", err);\n    return \"There was an error processing your request.\";\n  }\n\n  // postAsync: store the result in sharedState and return \"default\"\n  public async postAsync(sharedState: any, prepResult: string, execResult: string): Promise&lt;string&gt; {\n    const filename = this.params[\"filename\"] as string;\n    if (!sharedState.summary) {\n      sharedState.summary = {};\n    }\n    sharedState.summary[filename] = execResult;\n    return DEFAULT_ACTION; // or just \"default\"\n  }\n}\n\n// Example usage:\nconst summarizeNode = new SummarizeFile({ maxRetries: 3, wait: 10 });\nsummarizeNode.setParams({ filename: \"test_file.txt\" });\n\n// Prepare a shared state with data\nconst shared: any = {\n  data: {\n    \"test_file.txt\": \"Once upon a time in a faraway land...\",\n  },\n};\n\n// node.runAsync(...) calls prepAsync-&gt;execAsync-&gt;postAsync\n// If execAsync() fails repeatedly, it calls execFallbackAsync() before postAsync().\nsummarizeNode.runAsync(shared).then((actionReturned) =&gt; {\n  console.log(\"Action returned:\", actionReturned);  // Usually \"default\"\n  console.log(\"Summary stored:\", shared.summary?.[\"test_file.txt\"]);\n}).catch((error) =&gt; {\n  console.error(\"Node execution error:\", error);\n});\n</code></pre>"},{"location":"node/#explanation","title":"Explanation","text":"<ol> <li>prepAsync(sharedState) grabs the file content from <code>sharedState.data[filename]</code>.  </li> <li>execAsync(prepResult) calls an LLM to summarize the text.  </li> <li>Any error triggers retry logic. If all retries fail, execFallbackAsync() is called, returning a fallback value.  </li> <li>Finally, postAsync(sharedState, prepResult, execResult) writes the summary to <code>sharedState.summary</code> and returns <code>\"default\"</code> so the flow can continue.</li> </ol>"},{"location":"node/#summary","title":"Summary","text":"<p>In your <code>pocket.ts</code> framework, each Node inherits from <code>BaseNode</code> and may implement:</p> <ul> <li>prepAsync(): Gather/parse data from <code>sharedState</code>.  </li> <li>execAsync() (with optional retry logic): Execute the main logic, frequently an LLM call.  </li> <li>execFallbackAsync(): Provide a fallback result if <code>execAsync()</code> fails too many times.  </li> <li>postAsync(): Write results back to <code>sharedState</code>, decide next <code>Action</code>.</li> </ul> <p>This approach keeps your code organized while ensuring reliability, retryability, and a clear separation of concerns.  </p>"},{"location":"paradigm/","title":"Conceptual Paradigms","text":"<p>layout: default title: \"Paradigm\" nav_order: 4 has_children: true</p>"},{"location":"parallel/","title":"(Advanced) Parallel","text":"<p>Parallel Nodes and Flows let you run multiple Async Nodes and Flows concurrently\u2014for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute.</p> <p>In Node.js, JavaScript can only have true parallelism when using worker threads or multiple processes, but using async/await can still overlap I/O operations (e.g., LLM/network/database calls) effectively.</p>"},{"location":"parallel/#1-asyncparallelbatchnode","title":"1. AsyncParallelBatchNode","text":"<p>This concept is akin to an AsyncBatchNode but runs <code>execAsync()</code> in parallel for each item. Let's define a <code>ParallelSummaries</code> node that splits an array of texts, calls an LLM for each one in parallel, and then combines results:</p> <pre><code>import { AsyncParallelBatchNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\nimport { callLLM } from \"../path/to/your/llm-wrapper\";\n\nexport class ParallelSummaries extends AsyncParallelBatchNode&lt;string, string&gt; {\n  // prepAsync returns the array of items to process in parallel\n  public async prepAsync(sharedState: any): Promise&lt;string[]&gt; {\n    return sharedState.texts ?? [];\n  }\n\n  // execAsync is called in parallel for each item from prepAsync()\n  public async execAsync(text: string): Promise&lt;string&gt; {\n    const prompt = `Summarize: ${text}`;\n    return await callLLM(prompt); // LLM call\n  }\n\n  // postAsync collects the results into sharedState\n  public async postAsync(\n    sharedState: any,\n    prepResult: string[],\n    execResultList: string[]\n  ): Promise&lt;string&gt; {\n    sharedState.summary = execResultList.join(\"\\n\\n\");\n    return DEFAULT_ACTION; // continue or end flow as needed\n  }\n}\n\n// Example usage:\nconst node = new ParallelSummaries();\nconst flow = new Flow(node);\n\nconst shared: any = {\n  texts: [\n    \"Node.js is a JavaScript runtime built on Chrome's V8 engine.\",\n    \"TypeScript is a typed superset of JavaScript providing better tooling.\",\n    \"Parallel processing can reduce total latency for I/O-bound tasks.\"\n  ],\n};\n\nflow.runAsync(shared).then(() =&gt; {\n  console.log(\"All parallel summaries done.\");\n  console.log(\"Combined summary:\", shared.summary);\n});\n</code></pre> <p>Key Points: 1. prepAsync returns an array of texts. 2. execAsync is invoked in parallel for each text. 3. postAsync merges results (e.g. joins them as a single string).</p>"},{"location":"parallel/#2-asyncparallelbatchflow","title":"2. AsyncParallelBatchFlow","text":"<p>A parallel version of a BatchFlow, where each iteration of a sub-flow runs concurrently using different parameters. For example, if you have a LoadAndSummarizeFile flow, you can run it in parallel for multiple files at once.</p> <pre><code>import { AsyncParallelBatchFlow, Flow } from \"../src/pocket\";\nimport { LoadAndSummarizeFile } from \"./somewhere\";\n\nexport class SummarizeMultipleFiles extends AsyncParallelBatchFlow {\n  // We override prepAsync to produce a list of param objects\n  public async prepAsync(sharedState: any): Promise&lt;any[]&gt; {\n    // Return one param object per file\n    const files: string[] = sharedState.files ?? [];\n    return files.map((filename) =&gt; ({ filename }));\n  }\n}\n\n// Example usage:\nconst subFlow = new Flow(new LoadAndSummarizeFile());\nconst parallelFlow = new SummarizeMultipleFiles(subFlow);\n\nconst shared: any = {\n  files: [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"],\n};\n\nparallelFlow.runAsync(shared).then(() =&gt; {\n  console.log(\"All files processed in parallel!\");\n  // shared might now contain combined summaries or saved results per file\n});\n</code></pre> <p>Notes: 1. prepAsync returns an array of param objects. 2. Each item runs the <code>subFlow</code> at the same time with different parameters. 3. This is especially suitable for I/O-bound tasks like LLM calls or file operations.</p>"},{"location":"parallel/#best-practices","title":"Best Practices","text":"<ol> <li>Ensure Tasks Are Independent </li> <li> <p>Parallelizing tasks that share state or depend on each other can introduce conflicts or race conditions. Plan to keep each task's data separate or properly synchronized if needed.</p> </li> <li> <p>Watch Out for Rate Limits </p> </li> <li> <p>Parallel calls may quickly trigger rate limits on LLM services. You may need a throttling mechanism or concurrency limits (e.g. a semaphore).</p> </li> <li> <p>Leverage Batch APIs If Available </p> </li> <li> <p>Some LLM providers offer batch inference endpoints. This can be more efficient than launching many parallel requests and may help with rate-limit policies.</p> </li> <li> <p>Tune Concurrency </p> </li> <li> <p>In Node.js, concurrency techniques rely on asynchronous I/O. For CPU-bound tasks, consider Worker Threads or other processes.</p> </li> <li> <p>Clean Error Handling </p> </li> <li>If one parallel task fails, decide whether to continue or fail the entire flow. A custom error handling strategy may be needed for large-scale parallel tasks.</li> </ol>"},{"location":"preparation/","title":"Setup Steps","text":"<p>layout: default title: \"Details\" nav_order: 3 has_children: true</p>"},{"location":"rag/","title":"RAG (Retrieval Augmented Generation)","text":"<p>When building LLM applications that answer questions from a corpus of documents, you often need to: 1. Embed your documents and create a search index. 2. Retrieve relevant sections at query time. 3. Augment the LLM prompt with the retrieved context.</p> <p>Below is a two-Node flow in TypeScript that builds an embedding index and answers questions via a retrieval step.</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"../src/pocket\";\n\n/** \n * Placeholder for your embedding + index building code.\n * In a real scenario, you'd call a vector DB or embedding service.\n */\nasync function getEmbedding(text: string): Promise&lt;Float32Array&gt; {\n  // Return a 768-D or 1536-D embedding from a service or local model\n  // Here we just fake it by returning a vector of length = text.length\n  return new Float32Array(text.length).fill(0.5);\n}\nfunction createIndex(embeddings: Float32Array[]): any {\n  // Build or store the index in memory or an external DB\n  // Return an index object for subsequent searches\n  return { someIndexObject: true };\n}\nfunction searchIndex(index: any, queryEmbedding: Float32Array, topK: number): [Array&lt;[number, number]&gt;, any] {\n  // This function should return indices of the most relevant documents\n  // and potentially their scores. For demonstration, we'll assume it always returns\n  // an index of 0 with a dummy similarity score.\n  return [[[0, 0.99]], null];\n}\n\n/**\n * Placeholder for an LLM call. Replace with your actual logic, e.g. OpenAI chat API.\n */\nasync function callLLM(prompt: string): Promise&lt;string&gt; {\n  return `Answer to: ${prompt.substring(0, 60)}...`;\n}\n\n/**\n * Step 1: PrepareEmbeddingsNode\n *  - Gathers the corpus from sharedState[\"texts\"]\n *  - Creates embeddings\n *  - Builds a search index and stores it in sharedState[\"search_index\"]\n */\nexport class PrepareEmbeddingsNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string[]&gt; {\n    if (!Array.isArray(sharedState.texts)) {\n      throw new Error(\"sharedState.texts must be an array of strings\");\n    }\n    return sharedState.texts;\n  }\n\n  public async execAsync(texts: string[]): Promise&lt;any&gt; {\n    // Compute embeddings for each text\n    const embeddings: Float32Array[] = [];\n    for (let text of texts) {\n      const emb = await getEmbedding(text);\n      embeddings.push(emb);\n    }\n    // Create an index from these embeddings\n    const index = createIndex(embeddings);\n    return index;\n  }\n\n  public async postAsync(sharedState: any, prepResult: string[], execResult: any): Promise&lt;string&gt; {\n    // Store the search index\n    sharedState.search_index = execResult;\n    return DEFAULT_ACTION;\n  }\n}\n\n/**\n * Step 2: AnswerQuestionNode\n *  - Reads a question from the user or a passed-in param\n *  - Searches the index for the most relevant text\n *  - Calls the LLM with the question + relevant text to get an answer\n *  - Outputs the answer\n */\nexport class AnswerQuestionNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;{ question: string; relevantText: string }&gt; {\n    // For a real UI, you might read from the console or an API param\n    const question = this.params[\"question\"] ?? \"How does Node.js handle concurrency?\";\n\n    // Get embedding for this question\n    const questionEmbedding = await getEmbedding(question);\n\n    // Search the existing index\n    if (!sharedState.search_index) {\n      throw new Error(\"No search index found in sharedState\");\n    }\n    const [results] = searchIndex(sharedState.search_index, questionEmbedding, 1);\n    // results might look like: [[docIndex, similarityScore], ...], pick the top one\n    const docIndex = results[0][0]; // The top doc index\n    const relevantText = sharedState.texts?.[docIndex] ?? \"\";\n\n    return { question, relevantText };\n  }\n\n  public async execAsync({ question, relevantText }: { question: string; relevantText: string }): Promise&lt;string&gt; {\n    const prompt = `Question: ${question}\\nContext: ${relevantText}\\nAnswer: `;\n    return await callLLM(prompt);\n  }\n\n  public async postAsync(sharedState: any, prepResult: any, execResult: string): Promise&lt;string&gt; {\n    console.log(`Answer: ${execResult}`);\n    // In a real scenario, you might store the answer in sharedState or return a next Action\n    return DEFAULT_ACTION;  // Flow ends or continues\n  }\n}\n\n/**\n * Example usage:\n */\n(async () =&gt; {\n  const shared: any = {\n    texts: [\n      \"Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient.\",\n      \"TypeScript extends JavaScript by adding types, improving developer productivity.\",\n      \"Retrieval Augmented Generation (RAG) helps LLMs ground responses in real sources.\"\n    ]\n  };\n\n  // Create the nodes\n  const prepNode = new PrepareEmbeddingsNode();\n  const answerNode = new AnswerQuestionNode();\n  // Connect them\n  prepNode.addSuccessor(answerNode, \"default\");\n\n  // Build the Flow\n  const flow = new Flow(prepNode);\n\n  // Optionally set the question as a param for answerNode\n  answerNode.setParams({ question: \"What is Node.js concurrency model?\" });\n\n  // Run the flow\n  await flow.runAsync(shared);\n})();\n</code></pre> <p>Explanation:</p> <ol> <li>PrepareEmbeddingsNode:  </li> <li>Reads an array of texts from <code>sharedState.texts</code>.  </li> <li> <p>Computes embeddings and stores them in a new <code>sharedState.search_index</code>.  </p> </li> <li> <p>AnswerQuestionNode:  </p> </li> <li>Retrieves a question from <code>params[\"question\"]</code>.  </li> <li>Embeds the question, searches the index, fetches the top relevant text.  </li> <li>Calls the LLM with a prompt containing both the question and the relevant text.  </li> <li>Logs (or returns) the LLM's answer.</li> </ol> <p>This RAG approach ensures answers are grounded in a known corpus of documents, improving relevance and trustworthiness.</p>"},{"location":"structure/","title":"Structured Output","text":"<p>In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.</p> <p>Common approaches include:</p> <ul> <li>Prompting the LLM to strictly return a defined structure (often sufficient for modern LLMs).  </li> <li>Using LLMs or libraries that provide schema enforcement (e.g., OpenAI function calling, JSON schema validators).  </li> <li>Post-processing the LLM's response to extract structured content.</li> </ul>"},{"location":"structure/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Extracting Key Information</li> </ul> <pre><code>product:\n  name: Widget Pro\n  price: 199.99\n  description: |\n    A high-quality widget designed for professionals.\n    Suitable for advanced users.\n</code></pre> <ul> <li>Summarizing Documents into Bullet Points</li> </ul> <pre><code>summary:\n  - This product is easy to use.\n  - It is cost-effective.\n  - Suitable for novices and experts alike.\n</code></pre> <ul> <li>Generating Configuration Files</li> </ul> <pre><code>server:\n  host: 127.0.0.1\n  port: 8080\n  ssl: true\n</code></pre>"},{"location":"structure/#prompt-engineering","title":"Prompt Engineering","text":"<p>When prompting an LLM for structured output:</p> <ol> <li>Wrap the structure in code fences (e.g., ```yaml).  </li> <li>Validate that all required fields exist, and if absent or ill-formed, handle retries or cleanup in your Node logic.</li> </ol>"},{"location":"structure/#example-summarizing-text-in-yaml","title":"Example: Summarizing Text in YAML","text":"<p>Below is a TypeScript Node (<code>BaseNode</code>) demonstrating how to prompt an LLM for a YAML-based summary. It prompts for exactly 3 bullet points and then parses the LLM's response as YAML.</p> <pre><code>import { BaseNode, DEFAULT_ACTION } from \"../src/pocket\";\nimport { callLLM } from \"../path/to/your/llm-wrapper\";\n\n/**\n * SummarizeNode:\n * 1) Prepares a prompt with instructions for YAML output.\n * 2) Calls the LLM to generate the structured YAML.\n * 3) Parses the YAML and validates the result.\n */\nexport class SummarizeNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    // Grab the text to summarize\n    const textToSummarize: string = sharedState.text ?? \"No text provided.\";\n    return textToSummarize;\n  }\n\n  public async execAsync(text: string): Promise&lt;string&gt; {\n    // Construct a prompt that instructs the LLM to return exactly 3 bullet points in YAML\n    const prompt = `\nPlease summarize the following text in YAML with exactly 3 bullet points.\n\nText:\n${text}\n\nThe YAML should look like this:\n\n\\`\\`\\`yaml\nsummary:\n  - bullet 1\n  - bullet 2\n  - bullet 3\n\\`\\`\\`\n\nOnly return the YAML (including the fences).\n`;\n    // Call the LLM with your custom logic\n    const response = await callLLM(prompt);\n    return response;\n  }\n\n  public async postAsync(\n    sharedState: any,\n    prepResult: string,\n    llmResponse: string\n  ): Promise&lt;string&gt; {\n    // Extract the YAML content between the fences\n    let yamlStr = \"\";\n    try {\n      const startTag = \"```yaml\";\n      const endTag = \"```\";\n\n      const startIndex = llmResponse.indexOf(startTag);\n      const endIndex = llmResponse.indexOf(endTag, startIndex + startTag.length);\n\n      if (startIndex !== -1 &amp;&amp; endIndex !== -1) {\n        yamlStr = llmResponse.substring(startIndex + startTag.length, endIndex).trim();\n      } else {\n        throw new Error(\"LLM response did not contain valid ```yaml``` fences.\");\n      }\n\n      // Parse the YAML\n      // (You might need \"js-yaml\" or similar library for safe YAML parsing.)\n      const yaml = await import(\"js-yaml\");\n      const structResult = yaml.load(yamlStr);\n\n      // Validate the structure\n      if (!structResult || typeof structResult !== \"object\") {\n        throw new Error(\"Parsed result is not a valid YAML object.\");\n      }\n      if (!(\"summary\" in structResult) || !Array.isArray((structResult as any).summary)) {\n        throw new Error(\"Expected a 'summary' array in the YAML output.\");\n      }\n\n      // Save the structured output\n      sharedState.summary = structResult;\n    } catch (err) {\n      // Optionally retry or provide fallback. For now, just log and store an error.\n      console.error(\"Error parsing YAML from LLM:\", err);\n      sharedState.summary = { error: \"Invalid YAML output from LLM.\" };\n    }\n\n    return DEFAULT_ACTION; // Continue the flow\n  }\n}\n</code></pre> <p>How It Works: 1. prepAsync fetches the text from <code>sharedState</code>. 2. execAsync sends a strict YAML-form request to the LLM. 3. postAsync parses and validates the result. If invalid, you can handle it (e.g., retry or fallback).</p>"},{"location":"structure/#why-yaml-instead-of-json","title":"Why YAML instead of JSON?","text":"<p>YAML tends to be more LLM-friendly for multiline strings. In JSON, you often need to carefully escape quotes or newline characters. For example, a multi-line string in JSON has to store newlines as <code>\\n</code> and use escaped quotes like <code>\\\"</code>. With YAML, you can use the <code>|</code> block literal style to naturally preserve newlines and quotes.</p> <p>In JSON <pre><code>{\n  \"dialogue\": \"Alice said: \\\"Hello Bob.\\\\nHow are you?\\\\nI am good.\\\"\"\n}\n</code></pre> In YAML <pre><code>dialogue: |\n  Alice said: \"Hello Bob.\n  How are you?\n  I am good.\"\n</code></pre> No special escaping needed.</p>"},{"location":"structure/#summary","title":"Summary","text":"<ul> <li>Structured Outputs can make LLM calls more controllable and predictable.  </li> <li>Prompt carefully: specify the desired structure and syntax (e.g., YAML code fence).  </li> <li>Parse and validate the returned structure. If the LLM fails to comply, you can retry or use an alternative fallback.  </li> </ul> <p>With <code>pocket.ts</code>, you can integrate these steps into a Node-based flow, ensuring that each node is responsible for prompting, parsing, and validating data.</p>"},{"location":"tool/","title":"Tool","text":"<p>Similar to LLM wrappers, we don't provide built-in tools. Here, we recommend some minimal (and incomplete) implementations of commonly used tools. These examples can serve as a starting point for your own tooling.</p>"},{"location":"tool/#1-embedding-calls","title":"1. Embedding Calls","text":"<pre><code>def get_embedding(text):\n    from openai import OpenAI\n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    r = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return r.data[0].embedding\n\nget_embedding(\"What's the meaning of life?\")\n</code></pre>"},{"location":"tool/#2-vector-database-faiss","title":"2. Vector Database (Faiss)","text":"<pre><code>import faiss\nimport numpy as np\n\ndef create_index(embeddings):\n    dim = len(embeddings[0])\n    index = faiss.IndexFlatL2(dim)\n    index.add(np.array(embeddings).astype('float32'))\n    return index\n\ndef search_index(index, query_embedding, top_k=5):\n    D, I = index.search(\n        np.array([query_embedding]).astype('float32'), \n        top_k\n    )\n    return I, D\n\nindex = create_index(embeddings)\nsearch_index(index, query_embedding)\n</code></pre>"},{"location":"tool/#3-local-database","title":"3. Local Database","text":"<pre><code>import sqlite3\n\ndef execute_sql(query):\n    conn = sqlite3.connect(\"mydb.db\")\n    cursor = conn.cursor()\n    cursor.execute(query)\n    result = cursor.fetchall()\n    conn.commit()\n    conn.close()\n    return result\n</code></pre> <p>\u26a0\ufe0f Beware of SQL injection risk</p>"},{"location":"tool/#4-python-function-execution","title":"4. Python Function Execution","text":"<pre><code>def run_code(code_str):\n    env = {}\n    exec(code_str, env)\n    return env\n\nrun_code(\"print('Hello, world!')\")\n</code></pre> <p>\u26a0\ufe0f exec() is dangerous with untrusted input</p>"},{"location":"tool/#5-pdf-extraction","title":"5. PDF Extraction","text":"<p>If your PDFs are text-based, use PyMuPDF:</p> <pre><code>import fitz  # PyMuPDF\n\ndef extract_text(pdf_path):\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    doc.close()\n    return text\n\nextract_text(\"document.pdf\")\n</code></pre> <p>For image-based PDFs (e.g., scanned), OCR is needed. A easy and fast option is using an LLM with vision capabilities:</p> <pre><code>from openai import OpenAI\nimport base64\n\ndef call_llm_vision(prompt, image_data):\n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    img_base64 = base64.b64encode(image_data).decode('utf-8')\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \n                 \"image_url\": {\"url\": f\"data:image/png;base64,{img_base64}\"}}\n            ]\n        }]\n    )\n\n    return response.choices[0].message.content\n\npdf_document = fitz.open(\"document.pdf\")\npage_num = 0\npage = pdf_document[page_num]\npix = page.get_pixmap()\nimg_data = pix.tobytes(\"png\")\n\ncall_llm_vision(\"Extract text from this image\", img_data)\n</code></pre>"},{"location":"tool/#6-web-crawling","title":"6. Web Crawling","text":"<pre><code>def crawl_web(url):\n    import requests\n    from bs4 import BeautifulSoup\n    html = requests.get(url).text\n    soup = BeautifulSoup(html, \"html.parser\")\n    return soup.title.string, soup.get_text()\n</code></pre>"},{"location":"tool/#7-basic-search-serpapi-example","title":"7. Basic Search (SerpAPI example)","text":"<pre><code>def search_google(query):\n    import requests\n    params = {\n        \"engine\": \"google\",\n        \"q\": query,\n        \"api_key\": \"YOUR_API_KEY\"\n    }\n    r = requests.get(\"https://serpapi.com/search\", params=params)\n    return r.json()\n</code></pre>"},{"location":"tool/#8-audio-transcription-openai-whisper","title":"8. Audio Transcription (OpenAI Whisper)","text":"<pre><code>def transcribe_audio(file_path):\n    import openai\n    audio_file = open(file_path, \"rb\")\n    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n    return transcript[\"text\"]\n</code></pre>"},{"location":"tool/#9-text-to-speech-tts","title":"9. Text-to-Speech (TTS)","text":"<pre><code>def text_to_speech(text):\n    import pyttsx3\n    engine = pyttsx3.init()\n    engine.say(text)\n    engine.runAndWait()\n</code></pre>"},{"location":"tool/#10-sending-email","title":"10. Sending Email","text":"<pre><code>def send_email(to_address, subject, body, from_address, password):\n    import smtplib\n    from email.mime.text import MIMEText\n\n    msg = MIMEText(body)\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = from_address\n    msg[\"To\"] = to_address\n\n    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n        server.login(from_address, password)\n        server.sendmail(from_address, [to_address], msg.as_string())\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>This document demonstrates how to use pocket.ts in a typical TypeScript project. You can create Nodes, connect them in a Flow, and run that Flow with some shared state.</p>"},{"location":"usage/#1-install-import","title":"1. Install &amp; Import","text":"<pre><code>npm install pocket-ts\n</code></pre> <p>In your TypeScript code:</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"pocket-ts\"; \n// Adjust import if your local path is different or you have a monorepo structure\n</code></pre>"},{"location":"usage/#2-create-nodes","title":"2. Create Nodes","text":"<p>A Node extends <code>BaseNode</code> (or another node class) and implements any of the following async methods:</p> <ul> <li><code>prepAsync(sharedState: any)</code>: Prepare and return data for execution.</li> <li><code>execAsync(prepResult: any)</code>: Main logic, possibly calling an LLM or API.</li> <li><code>postAsync(sharedState: any, prepResult: any, execResult: any)</code>: Post-processing (e.g., saving results, deciding next action).</li> </ul> <pre><code>class GreetNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;string&gt; {\n    // Suppose we want to greet a user\n    const userName = sharedState.userName ?? \"Guest\";\n    return userName;\n  }\n\n  public async execAsync(userName: string): Promise&lt;string&gt; {\n    return `Hello, ${userName}!`;\n  }\n\n  public async postAsync(\n    sharedState: any,\n    prepResult: string,\n    execResult: string\n  ): Promise&lt;string&gt; {\n    sharedState.greeting = execResult;\n    console.log(\"GreetNode result:\", execResult);\n    return DEFAULT_ACTION; // proceed to next node or end if none\n  }\n}\n</code></pre>"},{"location":"usage/#3-connect-nodes-into-a-flow","title":"3. Connect Nodes into a Flow","text":"<p>You can chain nodes by specifying how one node transitions to the other. In <code>pocket.ts</code>, you do this via <code>addSuccessor(nextNode, actionString)</code>:</p> <pre><code>class AskFavoriteColorNode extends BaseNode {\n  // ...\n}\n\nclass RespondColorNode extends BaseNode {\n  // ...\n}\n\n// Create instances\nconst greetNode = new GreetNode();\nconst askColorNode = new AskFavoriteColorNode();\nconst respondColorNode = new RespondColorNode();\n\n// Connect\ngreetNode.addSuccessor(askColorNode, DEFAULT_ACTION);\naskColorNode.addSuccessor(respondColorNode, DEFAULT_ACTION);\n\n// Build a Flow that starts with greetNode\nconst flow = new Flow(greetNode);\n</code></pre>"},{"location":"usage/#4-run-the-flow","title":"4. Run the Flow","text":"<p>Create a <code>sharedState</code> object and pass it to the Flow's <code>runAsync</code> method:</p> <pre><code>(async () =&gt; {\n  const shared: any = {\n    userName: \"Alice\",\n  };\n\n  await flow.runAsync(shared);\n\n  // After the flow completes, inspect sharedState\n  console.log(\"All done! Final shared state:\", shared);\n})();\n</code></pre>"},{"location":"usage/#5-handling-multiple-actions","title":"5. Handling Multiple Actions","text":"<p>Sometimes, you'll want to branch based on different outcomes from a node. Each node's <code>postAsync</code> can return a string (often an \"Action\") which is used to choose the next node.</p> <pre><code>class SurveyNode extends BaseNode {\n  public async execAsync(_: unknown): Promise&lt;string&gt; {\n    const userInput = await this.askQuestion(\"Do you like TypeScript? (yes/no)\");\n    return userInput.trim().toLowerCase();\n  }\n\n  public async postAsync(\n    sharedState: any,\n    prepResult: unknown,\n    userResponse: string\n  ): Promise&lt;string&gt; {\n    if (userResponse === \"yes\") {\n      return \"yes_action\";\n    } else {\n      return \"no_action\";\n    }\n  }\n\n  private async askQuestion(question: string): Promise&lt;string&gt; {\n    // minimal prompt for demonstration\n    // replace with a real I/O method, or an LLM prompt\n    return Promise.resolve(\"yes\"); \n  }\n}\n\n// Usage:\n//     surveyNode.addSuccessor(yesNode, \"yes_action\");\n//     surveyNode.addSuccessor(noNode, \"no_action\");\n</code></pre>"},{"location":"usage/#6-retrying-error-handling","title":"6. Retrying &amp; Error Handling","text":"<p><code>BaseNode</code> and its subclasses can optionally accept retry parameters (e.g., <code>maxRetries</code> and <code>wait</code>). If <code>execAsync</code> fails, the node will automatically retry.</p> <pre><code>class UnreliableNode extends BaseNode {\n  constructor() {\n    // e.g., ask for 3 attempts with a 2-second wait\n    super({ maxRetries: 3, wait: 2 });\n  }\n\n  public async execAsync(_: unknown): Promise&lt;string&gt; {\n    if (Math.random() &lt; 0.5) {\n      throw new Error(\"Random failure!\");\n    }\n    return \"Success this time!\";\n  }\n}\n\n// If all retries fail, execFallbackAsync is called, if defined.\n// If fallback is not defined or rethrows, the flow can fail unless you handle it.\n</code></pre>"},{"location":"usage/#7-async-vs-sync","title":"7. Async vs Sync","text":"<p>Most LLM-based tasks or external API calls require async. With <code>pocket.ts</code>, <code>prepAsync</code>, <code>execAsync</code>, <code>postAsync</code>, and <code>execFallbackAsync</code> can all be asynchronous. You can handle synchronous tasks simply by returning a resolved Promise.</p>"},{"location":"usage/#8-example-putting-it-all-together","title":"8. Example: Putting It All Together","text":"<p>Below is a small end-to-end flow:</p> <pre><code>import { BaseNode, Flow, DEFAULT_ACTION } from \"pocket-ts\";\n\n// Node #1: Greet user\nclass GreetNode extends BaseNode {\n  public async execAsync(_: unknown): Promise&lt;string&gt; {\n    return \"Hello! What's your name?\";\n  }\n  public async postAsync(sharedState: any, _: any, greeting: string): Promise&lt;string&gt; {\n    console.log(greeting);\n    return DEFAULT_ACTION;\n  }\n}\n\n// Node #2: Get user's name (mock input)\nclass GetNameNode extends BaseNode {\n  public async execAsync(_: unknown): Promise&lt;string&gt; {\n    return \"Alice\"; // In real life, you'd ask or read from user input\n  }\n  public async postAsync(sharedState: any, _: any, name: string): Promise&lt;string&gt; {\n    sharedState.userName = name;\n    return DEFAULT_ACTION;\n  }\n}\n\n// Node #3: Personalize farewell\nclass FarewellNode extends BaseNode {\n  public async execAsync(_: unknown): Promise&lt;string&gt; {\n    return \"Nice to meet you!\";\n  }\n  public async postAsync(sharedState: any, _: any, farewell: string): Promise&lt;string&gt; {\n    console.log(`${farewell} Goodbye, ${sharedState.userName}!`);\n    return DEFAULT_ACTION;\n  }\n}\n\n// Build flow\nconst greetNode = new GreetNode();\nconst nameNode = new GetNameNode();\nconst farewellNode = new FarewellNode();\n\ngreetNode.addSuccessor(nameNode, DEFAULT_ACTION);\nnameNode.addSuccessor(farewellNode, DEFAULT_ACTION);\n\nconst flow = new Flow(greetNode);\n\n// Run flow\nflow.runAsync({}).then(() =&gt; {\n  console.log(\"Flow complete!\");\n});\n</code></pre> <p>Console output might look like: <pre><code>Hello! What's your name?\nNice to meet you! Goodbye, Alice!\nFlow complete!\n</code></pre></p>"},{"location":"usage/#summary","title":"Summary","text":"<ul> <li>Import <code>BaseNode</code> and <code>Flow</code> from <code>pocket.ts</code>.  </li> <li>Create Nodes with <code>prepAsync</code>, <code>execAsync</code>, <code>postAsync</code> as needed.  </li> <li>Chain nodes in a Flow and specify actions.  </li> <li>Run the flow using <code>runAsync(sharedState)</code>.  </li> <li>Optionally use features like retry, fallback, or action-based branching.  </li> </ul> <p>This is the basic usage pattern for building Node-based flows in <code>pocket.ts</code>. Feel free to explore more advanced features like parallel flows, multi-agent architectures, or specialized node types (batch nodes, etc.). </p>"},{"location":"viz/","title":"Visualization and Debugging","text":"<p>Similar to LLM wrappers, we don't provide built-in visualization or debugging. Below are minimal examples that can serve as a starting point for your own tooling.</p>"},{"location":"viz/#1-visualization-with-mermaid","title":"1. Visualization with Mermaid","text":"<p>The following TypeScript code recursively traverses your Nodes (and Flows, if you have them) to generate a Mermaid diagram syntax. It assigns unique IDs to each node, treats Flows as subgraphs, and creates edges (<code>--&gt;</code>) between nodes.</p> <pre><code>import { BaseNode, Flow } from \"../src/pocket\";\n\n/**\n * buildMermaid\n *\n * Recursively walks through a Flow or Node hierarchy\n * and returns a Mermaid \"graph LR\" diagram in string form.\n *\n * Assumes each Node has a \"successors\" property (Map&lt;string, BaseNode&gt;)\n * and each Flow might have \"start\" and (optionally) \"successors\" or sub-Flows.\n */\nexport function buildMermaid(start: BaseNode | Flow): string {\n  const lines: string[] = [\"graph LR\"];\n  const visited = new Set&lt;any&gt;();\n  const nodeIds = new Map&lt;any, string&gt;();\n  let counter = 1;\n\n  function getNodeId(obj: any): string {\n    if (!nodeIds.has(obj)) {\n      nodeIds.set(obj, `N${counter}`);\n      counter++;\n    }\n    return nodeIds.get(obj)!;\n  }\n\n  // A small helper to record \"A --&gt; B\" in the diagram\n  function link(aId: string, bId: string): void {\n    lines.push(`    ${aId} --&gt; ${bId}`);\n  }\n\n  // Attempt to retrieve a name for object (Node or Flow)\n  function getObjName(obj: any): string {\n    return obj?.constructor?.name || \"Unknown\";\n  }\n\n  function walk(current: any, parentId?: string): void {\n    if (visited.has(current)) {\n      // Already visited, just link them if we have a parent\n      if (parentId) {\n        link(parentId, getNodeId(current));\n      }\n      return;\n    }\n    visited.add(current);\n\n    if (current instanceof Flow) {\n      // If it's a Flow, represent it as a subgraph\n      const flowId = getNodeId(current);\n      const flowName = getObjName(current);\n      lines.push(`\\n    subgraph sub_flow_${flowId}[${flowName}]`);\n\n      // If the Flow has a \"start\" node, link from parent to start\n      if (current.start) {\n        if (parentId) {\n          // Connect the parent to the start node\n          link(parentId, getNodeId(current.start));\n        }\n        // Recursively visit the flow's start node\n        walk(current.start, undefined);\n      }\n\n      // If the Flow has its own successors logic, handle that here\n      // In many frameworks, a Flow might not have direct \"successors\"\n      // but may embed sub-nodes or sub-flows. Adapt to your structure.\n\n      // End subgraph\n      lines.push(`    end\\n`);\n    } else if (current instanceof BaseNode) {\n      // It's a Node\n      const nodeId = getNodeId(current);\n      const nodeName = getObjName(current);\n      lines.push(`    ${nodeId}[\"${nodeName}\"]`);\n\n      // Link from parent\n      if (parentId) {\n        link(parentId, nodeId);\n      }\n\n      // Recursively traverse this node's successors\n      // Assume a \"successors\" Map&lt;string, BaseNode&gt; on your Node\n      if (current.successors) {\n        for (const nxt of current.successors.values()) {\n          walk(nxt, nodeId);\n        }\n      }\n    } else {\n      // Unrecognized node type\n      // Might be an error or a data object\n      // You could skip or throw an error\n    }\n  }\n\n  walk(start);\n  return lines.join(\"\\n\");\n}\n</code></pre>"},{"location":"viz/#example-usage","title":"Example Usage","text":"<pre><code>import { BaseNode, Flow } from \"../src/pocket\";\nimport { buildMermaid } from \"./buildMermaidSnippet\"; // your file location\n\nclass DataPrepBatchNode extends BaseNode {}\nclass ValidateDataNode extends BaseNode {}\nclass FeatureExtractionNode extends BaseNode {}\nclass TrainModelNode extends BaseNode {}\nclass EvaluateModelNode extends BaseNode {}\nclass ModelFlow extends Flow {}\nclass DataScienceFlow extends Flow {}\n\nconst featureNode = new FeatureExtractionNode();\nconst trainNode = new TrainModelNode();\nconst evaluateNode = new EvaluateModelNode();\nfeatureNode.addSuccessor(trainNode, \"default\");\ntrainNode.addSuccessor(evaluateNode, \"default\");\n\nconst modelFlow = new ModelFlow(featureNode);\n\nconst dataPrepNode = new DataPrepBatchNode();\nconst validateNode = new ValidateDataNode();\ndataPrepNode.addSuccessor(validateNode, \"default\");\nvalidateNode.addSuccessor(modelFlow, \"default\");\n\nconst dataScienceFlow = new DataScienceFlow(dataPrepNode);\n\n// Build a mermaid diagram\nconst mermaidSyntax = buildMermaid(dataScienceFlow);\nconsole.log(\"Mermaid diagram:\\n\", mermaidSyntax);\n</code></pre> <p>You might get output like:</p> <pre><code>graph LR\n    subgraph sub_flow_N1[DataScienceFlow]\n    N2[\"DataPrepBatchNode\"]\n    N3[\"ValidateDataNode\"]\n    N2 --&gt; N3\n    subgraph sub_flow_N4[ModelFlow]\n    N5[\"FeatureExtractionNode\"]\n    N6[\"TrainModelNode\"]\n    N5 --&gt; N6\n    N7[\"EvaluateModelNode\"]\n    N6 --&gt; N7\n    end\n\n    N3 --&gt; N4\n    end\n</code></pre> <p>Which can be rendered as a Mermaid diagram.</p>"},{"location":"viz/#2-call-stack-debugging","title":"2. Call Stack Debugging","text":"<p>In Node.js/TypeScript, you don't typically have direct access to Python's <code>inspect</code> module or frames. However, you can capture stack traces by creating an <code>Error</code> object and analyzing its <code>.stack</code> property, or simply log your Node transitions in <code>prepAsync</code>, <code>execAsync</code>, or <code>postAsync</code> with relevant info.</p>"},{"location":"viz/#example-stack-capture","title":"Example Stack Capture","text":"<pre><code>/**\n * getNodeCallStack\n *\n * Minimal demonstration of capturing a call stack in Node.js.\n * This does NOT necessarily show each Node in the call chain,\n * but it can let you see the file and line references.\n */\nexport function getNodeCallStack(): string[] {\n  // Create a new error to capture the stack\n  const err = new Error(\"Stack capture\");\n  const lines = err.stack ? err.stack.split(\"\\n\").slice(2) : [];\n  return lines.map((line) =&gt; line.trim());\n}\n\n// Example usage in a Node:\nimport { BaseNode } from \"../src/pocket\";\n\nclass EvaluateModelNode extends BaseNode {\n  public async prepAsync(sharedState: any): Promise&lt;void&gt; {\n    const stack = getNodeCallStack();\n    console.log(\"Call stack lines:\\n\", stack.join(\"\\n\"));\n  }\n}\n</code></pre> <p>Note: This will show you the JS/TS call stack at the moment. If you want to track which Nodes have been called so far, a simpler approach is to maintain a custom log in <code>sharedState</code> (or a global logger) and push an entry each time a Node is <code>prep</code>-ed, <code>exec</code>-ed, or <code>post</code>-ed. Then you can see the node call chain in the order they were invoked.</p>"},{"location":"viz/#summary","title":"Summary","text":"<ul> <li>Mermaid Visualization: The snippet above traverses flows and nodes to output a simple <code>graph LR</code> diagram. Customize it to reflect your actual Node/Flow structures or more advanced sub-flow logic.  </li> <li>Call Stack Debugging: In TypeScript, using <code>Error().stack</code> is one approach, though typically you might prefer structured logs or events to track your node transitions.  </li> </ul> <p>Combined, these can help you better understand and debug complex multi-node flows in <code>pocket.ts</code>.</p>"}]}